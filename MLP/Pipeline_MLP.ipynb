{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'feature_selection' from 'E:\\\\Reconhecimento de Padrões\\\\pattern_recognition\\\\MLP\\\\feature_selection.py'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import das libs necessárias\n",
    "import pandas as pd # trabalhar com dataframes\n",
    "import numpy as np # realizacao de algumas operacoes com matrizes\n",
    "\n",
    "#imagens\n",
    "import cv2 # transformacoes faceis em imagens\n",
    "from PIL import Image # trabalhar com imagens\n",
    "\n",
    "# ferramentas\n",
    "import glob # exploracao de diretorios\n",
    "from pylab import *\n",
    "import tqdm\n",
    "import importlib\n",
    "import time\n",
    "\n",
    "# plot \n",
    "import matplotlib.pyplot as plt # plotagem\n",
    "%matplotlib inline\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.neural_network import MLPClassifier \n",
    "from sklearn.model_selection import GridSearchCV # Grid Search para rodar todos os parametros\n",
    "\n",
    "#importando ferramentas já implementadas anteriormente\n",
    "import sampling #retorna uma amostra para treino\n",
    "importlib.reload(sampling)\n",
    "import PCA_and_Inverse\n",
    "importlib.reload(PCA_and_Inverse)\n",
    "import feature_selection\n",
    "importlib.reload(feature_selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cria uma amostra das imagens do tamanho definido e carrega em dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 400/400 [00:00<00:00, 815.15it/s]\n"
     ]
    }
   ],
   "source": [
    "#carregando amostra dos dados\n",
    "X, Y = sampling.get_sample(400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normaliza os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sampling.get_min_max_normalization(data=X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extração de características\n",
    "\n",
    "- Calcula as Componentes principais para os dados carregados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA_and_Inverse.PCA_and_Inverse()\n",
    "pca_components = pca.get_PCA(X=X, n_components=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seleção de características\n",
    "\n",
    "- Calcula as melhores features a serem utilizadas pelos métodos de:\n",
    "    - Informação Mútua\n",
    "    - Algoritmo Genético"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Informação Mútua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py:752: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "mic_threshold = 0.05\n",
    "mic_features_df = feature_selection.get_MIC_features(X=X, Y=Y)\n",
    "mic_features = list(mic_features_df[mic_features_df[0] > mic_threshold].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algoritmo Genético"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features Shape: (300, 160000)\n",
      "Training Labels Shape: (300, 1)\n",
      "Testing Features Shape: (100, 160000)\n",
      "Testing Labels Shape: (100, 1)\n",
      "Baseline AUC 0.5112179487179487\n",
      "Optimizing with 1000 generations\n",
      "Generation 0\n",
      "Best result : 0.6065705128205129\n",
      "Generation 1\n",
      "Best result : 0.6065705128205129\n",
      "Generation 2\n",
      "Best result : 0.6065705128205129\n",
      "Generation 3\n",
      "Best result : 0.6065705128205129\n",
      "Generation 4\n",
      "Best result : 0.6065705128205129\n",
      "Generation 5\n",
      "Best result : 0.6065705128205129\n",
      "Generation 6\n",
      "Best result : 0.6065705128205129\n",
      "Generation 7\n",
      "Best result : 0.6065705128205129\n",
      "Generation 8\n",
      "Best result : 0.6065705128205129\n",
      "Generation 9\n",
      "Best result : 0.6065705128205129\n",
      "Generation 10\n",
      "Best result : 0.6065705128205129\n",
      "Generation 11\n",
      "Best result : 0.6065705128205129\n",
      "Generation 12\n",
      "Best result : 0.6065705128205129\n",
      "Generation 13\n",
      "Best result : 0.6065705128205129\n",
      "Generation 14\n",
      "Best result : 0.6065705128205129\n",
      "Generation 15\n",
      "Best result : 0.6065705128205129\n",
      "Generation 16\n",
      "Best result : 0.6065705128205129\n",
      "Generation 17\n",
      "Best result : 0.6065705128205129\n",
      "Generation 18\n",
      "Best result : 0.6065705128205129\n",
      "Generation 19\n",
      "Best result : 0.6065705128205129\n",
      "Generation 20\n",
      "Best result : 0.6065705128205129\n",
      "Generation 21\n",
      "Best result : 0.6065705128205129\n",
      "Generation 22\n",
      "Best result : 0.6065705128205129\n",
      "Generation 23\n",
      "Best result : 0.6065705128205129\n",
      "Generation 24\n",
      "Best result : 0.6169871794871794\n",
      "Generation 25\n",
      "Best result : 0.6169871794871794\n",
      "Generation 26\n",
      "Best result : 0.6169871794871794\n",
      "Generation 27\n",
      "Best result : 0.6169871794871794\n",
      "Generation 28\n",
      "Best result : 0.6169871794871794\n",
      "Generation 29\n",
      "Best result : 0.6169871794871794\n",
      "Generation 30\n",
      "Best result : 0.6169871794871794\n",
      "Generation 31\n",
      "Best result : 0.6169871794871794\n",
      "Generation 32\n",
      "Best result : 0.6169871794871794\n",
      "Generation 33\n",
      "Best result : 0.6602564102564102\n",
      "Generation 34\n",
      "Best result : 0.6602564102564102\n",
      "Generation 35\n",
      "Best result : 0.6602564102564102\n",
      "Generation 36\n",
      "Best result : 0.6602564102564102\n",
      "Generation 37\n",
      "Best result : 0.6602564102564102\n",
      "Generation 38\n",
      "Best result : 0.6602564102564102\n",
      "Generation 39\n",
      "Best result : 0.6602564102564102\n",
      "Generation 40\n",
      "Best result : 0.6602564102564102\n",
      "Generation 41\n",
      "Best result : 0.6602564102564102\n",
      "Generation 42\n",
      "Best result : 0.6602564102564102\n",
      "Generation 43\n",
      "Best result : 0.6602564102564102\n",
      "Generation 44\n",
      "Best result : 0.6602564102564102\n",
      "Generation 45\n",
      "Best result : 0.6602564102564102\n",
      "Generation 46\n",
      "Best result : 0.6602564102564102\n",
      "Generation 47\n",
      "Best result : 0.6602564102564102\n",
      "Generation 48\n",
      "Best result : 0.6602564102564102\n",
      "Generation 49\n",
      "Best result : 0.6602564102564102\n",
      "Generation 50\n",
      "Best result : 0.6602564102564102\n",
      "Generation 51\n",
      "Best result : 0.6602564102564102\n",
      "Generation 52\n",
      "Best result : 0.6602564102564102\n",
      "Generation 53\n",
      "Best result : 0.6602564102564102\n",
      "Generation 54\n",
      "Best result : 0.6602564102564102\n",
      "Generation 55\n",
      "Best result : 0.6602564102564102\n",
      "Generation 56\n",
      "Best result : 0.6602564102564102\n",
      "Generation 57\n",
      "Best result : 0.6602564102564102\n",
      "Generation 58\n",
      "Best result : 0.6602564102564102\n",
      "Generation 59\n",
      "Best result : 0.6602564102564102\n",
      "Generation 60\n",
      "Best result : 0.6602564102564102\n",
      "Generation 61\n",
      "Best result : 0.6602564102564102\n",
      "Generation 62\n",
      "Best result : 0.6602564102564102\n",
      "Generation 63\n",
      "Best result : 0.6602564102564102\n",
      "Generation 64\n",
      "Best result : 0.6602564102564102\n",
      "Generation 65\n",
      "Best result : 0.6602564102564102\n",
      "Generation 66\n",
      "Best result : 0.6602564102564102\n",
      "Generation 67\n",
      "Best result : 0.6602564102564102\n",
      "Generation 68\n",
      "Best result : 0.6602564102564102\n",
      "Generation 69\n",
      "Best result : 0.6602564102564102\n",
      "Generation 70\n",
      "Best result : 0.6602564102564102\n",
      "Generation 71\n",
      "Best result : 0.6602564102564102\n",
      "Generation 72\n",
      "Best result : 0.6602564102564102\n",
      "Generation 73\n",
      "Best result : 0.6602564102564102\n",
      "Generation 74\n",
      "Best result : 0.6602564102564102\n",
      "Generation 75\n",
      "Best result : 0.6602564102564102\n",
      "Generation 76\n",
      "Best result : 0.6602564102564102\n",
      "Generation 77\n",
      "Best result : 0.6602564102564102\n",
      "Generation 78\n",
      "Best result : 0.6602564102564102\n",
      "Generation 79\n",
      "Best result : 0.6602564102564102\n",
      "Generation 80\n",
      "Best result : 0.6602564102564102\n",
      "Generation 81\n",
      "Best result : 0.6602564102564102\n",
      "Generation 82\n",
      "Best result : 0.6602564102564102\n",
      "Generation 83\n",
      "Best result : 0.6602564102564102\n",
      "Generation 84\n",
      "Best result : 0.6602564102564102\n",
      "Generation 85\n",
      "Best result : 0.6602564102564102\n",
      "Generation 86\n",
      "Best result : 0.6602564102564102\n",
      "Generation 87\n",
      "Best result : 0.6602564102564102\n",
      "Generation 88\n",
      "Best result : 0.6602564102564102\n",
      "Generation 89\n",
      "Best result : 0.6602564102564102\n",
      "Generation 90\n",
      "Best result : 0.6602564102564102\n",
      "Generation 91\n",
      "Best result : 0.6602564102564102\n",
      "Generation 92\n",
      "Best result : 0.6602564102564102\n",
      "Generation 93\n",
      "Best result : 0.6602564102564102\n",
      "Generation 94\n",
      "Best result : 0.6602564102564102\n",
      "Generation 95\n",
      "Best result : 0.6602564102564102\n",
      "Generation 96\n",
      "Best result : 0.6602564102564102\n",
      "Generation 97\n",
      "Best result : 0.6602564102564102\n",
      "Generation 98\n",
      "Best result : 0.6602564102564102\n",
      "Generation 99\n",
      "Best result : 0.6602564102564102\n",
      "Generation 100\n",
      "Best result : 0.6602564102564102\n",
      "Generation 101\n",
      "Best result : 0.6602564102564102\n",
      "Generation 102\n",
      "Best result : 0.6602564102564102\n",
      "Generation 103\n",
      "Best result : 0.6602564102564102\n",
      "Generation 104\n",
      "Best result : 0.6602564102564102\n",
      "Generation 105\n",
      "Best result : 0.6602564102564102\n",
      "Generation 106\n",
      "Best result : 0.6602564102564102\n",
      "Generation 107\n",
      "Best result : 0.6602564102564102\n",
      "Generation 108\n",
      "Best result : 0.6602564102564102\n",
      "Generation 109\n",
      "Best result : 0.6602564102564102\n",
      "Generation 110\n",
      "Best result : 0.6602564102564102\n",
      "Generation 111\n",
      "Best result : 0.6602564102564102\n",
      "Generation 112\n",
      "Best result : 0.6602564102564102\n",
      "Generation 113\n",
      "Best result : 0.6602564102564102\n",
      "Generation 114\n",
      "Best result : 0.6602564102564102\n",
      "Generation 115\n",
      "Best result : 0.6602564102564102\n",
      "Generation 116\n",
      "Best result : 0.6602564102564102\n",
      "Generation 117\n",
      "Best result : 0.6602564102564102\n",
      "Generation 118\n",
      "Best result : 0.6602564102564102\n",
      "Generation 119\n",
      "Best result : 0.6602564102564102\n",
      "Generation 120\n",
      "Best result : 0.6602564102564102\n",
      "Generation 121\n",
      "Best result : 0.6602564102564102\n",
      "Generation 122\n",
      "Best result : 0.6602564102564102\n",
      "Generation 123\n",
      "Best result : 0.6602564102564102\n",
      "Generation 124\n",
      "Best result : 0.6602564102564102\n",
      "Generation 125\n",
      "Best result : 0.6602564102564102\n",
      "Generation 126\n",
      "Best result : 0.6602564102564102\n",
      "Generation 127\n",
      "Best result : 0.6602564102564102\n",
      "Generation 128\n",
      "Best result : 0.6602564102564102\n",
      "Generation 129\n",
      "Best result : 0.6602564102564102\n",
      "Generation 130\n",
      "Best result : 0.6602564102564102\n",
      "Generation 131\n",
      "Best result : 0.6602564102564102\n",
      "Generation 132\n",
      "Best result : 0.6602564102564102\n",
      "Generation 133\n",
      "Best result : 0.6602564102564102\n",
      "Generation 134\n",
      "Best result : 0.6602564102564102\n",
      "Generation 135\n",
      "Best result : 0.6602564102564102\n",
      "Generation 136\n",
      "Best result : 0.6602564102564102\n",
      "Generation 137\n",
      "Best result : 0.6602564102564102\n",
      "Generation 138\n",
      "Best result : 0.6602564102564102\n",
      "Generation 139\n",
      "Best result : 0.6602564102564102\n",
      "Generation 140\n",
      "Best result : 0.6602564102564102\n",
      "Generation 141\n",
      "Best result : 0.6602564102564102\n",
      "Generation 142\n",
      "Best result : 0.6602564102564102\n",
      "Generation 143\n",
      "Best result : 0.6602564102564102\n",
      "Generation 144\n",
      "Best result : 0.6602564102564102\n",
      "Generation 145\n",
      "Best result : 0.6602564102564102\n",
      "Generation 146\n",
      "Best result : 0.6602564102564102\n",
      "Generation 147\n",
      "Best result : 0.6602564102564102\n",
      "Generation 148\n",
      "Best result : 0.6602564102564102\n",
      "Generation 149\n",
      "Best result : 0.6602564102564102\n",
      "Generation 150\n",
      "Best result : 0.6602564102564102\n",
      "Generation 151\n",
      "Best result : 0.6602564102564102\n",
      "Generation 152\n",
      "Best result : 0.6602564102564102\n",
      "Generation 153\n",
      "Best result : 0.6602564102564102\n",
      "Generation 154\n",
      "Best result : 0.6602564102564102\n",
      "Generation 155\n",
      "Best result : 0.6602564102564102\n",
      "Generation 156\n",
      "Best result : 0.6602564102564102\n",
      "Generation 157\n",
      "Best result : 0.6602564102564102\n",
      "Generation 158\n",
      "Best result : 0.6602564102564102\n",
      "Generation 159\n",
      "Best result : 0.6602564102564102\n",
      "Generation 160\n",
      "Best result : 0.6602564102564102\n",
      "Generation 161\n",
      "Best result : 0.6602564102564102\n",
      "Generation 162\n",
      "Best result : 0.6602564102564102\n",
      "Generation 163\n",
      "Best result : 0.6602564102564102\n",
      "Generation 164\n",
      "Best result : 0.6602564102564102\n",
      "Generation 165\n",
      "Best result : 0.6602564102564102\n",
      "Generation 166\n",
      "Best result : 0.6602564102564102\n",
      "Generation 167\n",
      "Best result : 0.6602564102564102\n",
      "Generation 168\n",
      "Best result : 0.6602564102564102\n",
      "Generation 169\n",
      "Best result : 0.6602564102564102\n",
      "Generation 170\n",
      "Best result : 0.6602564102564102\n",
      "Generation 171\n",
      "Best result : 0.6602564102564102\n",
      "Generation 172\n",
      "Best result : 0.6602564102564102\n",
      "Generation 173\n",
      "Best result : 0.6602564102564102\n",
      "Generation 174\n",
      "Best result : 0.6602564102564102\n",
      "Generation 175\n",
      "Best result : 0.6602564102564102\n",
      "Generation 176\n",
      "Best result : 0.6602564102564102\n",
      "Generation 177\n",
      "Best result : 0.6602564102564102\n",
      "Generation 178\n",
      "Best result : 0.6602564102564102\n",
      "Generation 179\n",
      "Best result : 0.6602564102564102\n",
      "Generation 180\n",
      "Best result : 0.6602564102564102\n",
      "Generation 181\n",
      "Best result : 0.6602564102564102\n",
      "Generation 182\n",
      "Best result : 0.6602564102564102\n",
      "Generation 183\n",
      "Best result : 0.6602564102564102\n",
      "Generation 184\n",
      "Best result : 0.6602564102564102\n",
      "Generation 185\n",
      "Best result : 0.6602564102564102\n",
      "Generation 186\n",
      "Best result : 0.6602564102564102\n",
      "Generation 187\n",
      "Best result : 0.6602564102564102\n",
      "Generation 188\n",
      "Best result : 0.6602564102564102\n",
      "Generation 189\n",
      "Best result : 0.6602564102564102\n",
      "Generation 190\n",
      "Best result : 0.6602564102564102\n",
      "Generation 191\n",
      "Best result : 0.6602564102564102\n",
      "Generation 192\n",
      "Best result : 0.6602564102564102\n",
      "Generation 193\n",
      "Best result : 0.6602564102564102\n",
      "Generation 194\n",
      "Best result : 0.6602564102564102\n",
      "Generation 195\n",
      "Best result : 0.6602564102564102\n",
      "Generation 196\n",
      "Best result : 0.6602564102564102\n",
      "Generation 197\n",
      "Best result : 0.6602564102564102\n",
      "Generation 198\n",
      "Best result : 0.6602564102564102\n",
      "Generation 199\n",
      "Best result : 0.6602564102564102\n",
      "Generation 200\n",
      "Best result : 0.6602564102564102\n",
      "Generation 201\n",
      "Best result : 0.6602564102564102\n",
      "Generation 202\n",
      "Best result : 0.6602564102564102\n",
      "Generation 203\n",
      "Best result : 0.6602564102564102\n",
      "Generation 204\n",
      "Best result : 0.6602564102564102\n",
      "Generation 205\n",
      "Best result : 0.6602564102564102\n",
      "Generation 206\n",
      "Best result : 0.6602564102564102\n",
      "Generation 207\n",
      "Best result : 0.6602564102564102\n",
      "Generation 208\n",
      "Best result : 0.6602564102564102\n",
      "Generation 209\n",
      "Best result : 0.6602564102564102\n",
      "Generation 210\n",
      "Best result : 0.6602564102564102\n",
      "Generation 211\n",
      "Best result : 0.6602564102564102\n",
      "Generation 212\n",
      "Best result : 0.6602564102564102\n",
      "Generation 213\n",
      "Best result : 0.6602564102564102\n",
      "Generation 214\n",
      "Best result : 0.6602564102564102\n",
      "Generation 215\n",
      "Best result : 0.6602564102564102\n",
      "Generation 216\n",
      "Best result : 0.6602564102564102\n",
      "Generation 217\n",
      "Best result : 0.6602564102564102\n",
      "Generation 218\n",
      "Best result : 0.6602564102564102\n",
      "Generation 219\n",
      "Best result : 0.6602564102564102\n",
      "Generation 220\n",
      "Best result : 0.6602564102564102\n",
      "Generation 221\n",
      "Best result : 0.6602564102564102\n",
      "Generation 222\n",
      "Best result : 0.6602564102564102\n",
      "Generation 223\n",
      "Best result : 0.6602564102564102\n",
      "Generation 224\n",
      "Best result : 0.6602564102564102\n",
      "Generation 225\n",
      "Best result : 0.6602564102564102\n",
      "Generation 226\n",
      "Best result : 0.6602564102564102\n",
      "Generation 227\n",
      "Best result : 0.6602564102564102\n",
      "Generation 228\n",
      "Best result : 0.6602564102564102\n",
      "Generation 229\n",
      "Best result : 0.6602564102564102\n",
      "Generation 230\n",
      "Best result : 0.6602564102564102\n",
      "Generation 231\n",
      "Best result : 0.6602564102564102\n",
      "Generation 232\n",
      "Best result : 0.6602564102564102\n",
      "Generation 233\n",
      "Best result : 0.6602564102564102\n",
      "Generation 234\n",
      "Best result : 0.6602564102564102\n",
      "Generation 235\n",
      "Best result : 0.6602564102564102\n",
      "Generation 236\n",
      "Best result : 0.6602564102564102\n",
      "Generation 237\n",
      "Best result : 0.6602564102564102\n",
      "Generation 238\n",
      "Best result : 0.6602564102564102\n",
      "Generation 239\n",
      "Best result : 0.6602564102564102\n",
      "Generation 240\n",
      "Best result : 0.6602564102564102\n",
      "Generation 241\n",
      "Best result : 0.6602564102564102\n",
      "Generation 242\n",
      "Best result : 0.6602564102564102\n",
      "Generation 243\n",
      "Best result : 0.6602564102564102\n",
      "Generation 244\n",
      "Best result : 0.6602564102564102\n",
      "Generation 245\n",
      "Best result : 0.6602564102564102\n",
      "Generation 246\n",
      "Best result : 0.6602564102564102\n",
      "Generation 247\n",
      "Best result : 0.6602564102564102\n",
      "Generation 248\n",
      "Best result : 0.6602564102564102\n",
      "Generation 249\n",
      "Best result : 0.6602564102564102\n",
      "Generation 250\n",
      "Best result : 0.6602564102564102\n",
      "Generation 251\n",
      "Best result : 0.6602564102564102\n",
      "Generation 252\n",
      "Best result : 0.6602564102564102\n",
      "Generation 253\n",
      "Best result : 0.6602564102564102\n",
      "Generation 254\n",
      "Best result : 0.6602564102564102\n",
      "Generation 255\n",
      "Best result : 0.6602564102564102\n",
      "Generation 256\n",
      "Best result : 0.6602564102564102\n",
      "Generation 257\n",
      "Best result : 0.6602564102564102\n",
      "Generation 258\n",
      "Best result : 0.6602564102564102\n",
      "Generation 259\n",
      "Best result : 0.6602564102564102\n",
      "Generation 260\n",
      "Best result : 0.6602564102564102\n",
      "Generation 261\n",
      "Best result : 0.6602564102564102\n",
      "Generation 262\n",
      "Best result : 0.6602564102564102\n",
      "Generation 263\n",
      "Best result : 0.6602564102564102\n",
      "Generation 264\n",
      "Best result : 0.6602564102564102\n",
      "Generation 265\n",
      "Best result : 0.6602564102564102\n",
      "Generation 266\n",
      "Best result : 0.6602564102564102\n",
      "Generation 267\n",
      "Best result : 0.6602564102564102\n",
      "Generation 268\n",
      "Best result : 0.6602564102564102\n",
      "Generation 269\n",
      "Best result : 0.6602564102564102\n",
      "Generation 270\n",
      "Best result : 0.6602564102564102\n",
      "Generation 271\n",
      "Best result : 0.6602564102564102\n",
      "Generation 272\n",
      "Best result : 0.6602564102564102\n",
      "Generation 273\n",
      "Best result : 0.6602564102564102\n",
      "Generation 274\n",
      "Best result : 0.6602564102564102\n",
      "Generation 275\n",
      "Best result : 0.6602564102564102\n",
      "Generation 276\n",
      "Best result : 0.6602564102564102\n",
      "Generation 277\n",
      "Best result : 0.6602564102564102\n",
      "Generation 278\n",
      "Best result : 0.6602564102564102\n",
      "Generation 279\n",
      "Best result : 0.6602564102564102\n",
      "Generation 280\n",
      "Best result : 0.6602564102564102\n",
      "Generation 281\n",
      "Best result : 0.6602564102564102\n",
      "Generation 282\n",
      "Best result : 0.6602564102564102\n",
      "Generation 283\n",
      "Best result : 0.6602564102564102\n",
      "Generation 284\n",
      "Best result : 0.6602564102564102\n",
      "Generation 285\n",
      "Best result : 0.6602564102564102\n",
      "Generation 286\n",
      "Best result : 0.6602564102564102\n",
      "Generation 287\n",
      "Best result : 0.6602564102564102\n",
      "Generation 288\n",
      "Best result : 0.6602564102564102\n",
      "Generation 289\n",
      "Best result : 0.6602564102564102\n",
      "Generation 290\n",
      "Best result : 0.6602564102564102\n",
      "Generation 291\n",
      "Best result : 0.6602564102564102\n",
      "Generation 292\n",
      "Best result : 0.6602564102564102\n",
      "Generation 293\n",
      "Best result : 0.6602564102564102\n",
      "Generation 294\n",
      "Best result : 0.6602564102564102\n",
      "Generation 295\n",
      "Best result : 0.6602564102564102\n",
      "Generation 296\n",
      "Best result : 0.6602564102564102\n",
      "Generation 297\n",
      "Best result : 0.6602564102564102\n",
      "Generation 298\n",
      "Best result : 0.6602564102564102\n",
      "Generation 299\n",
      "Best result : 0.6602564102564102\n",
      "Generation 300\n",
      "Best result : 0.6602564102564102\n",
      "Generation 301\n",
      "Best result : 0.6602564102564102\n",
      "Generation 302\n",
      "Best result : 0.6602564102564102\n",
      "Generation 303\n",
      "Best result : 0.6602564102564102\n",
      "Generation 304\n",
      "Best result : 0.6602564102564102\n",
      "Generation 305\n",
      "Best result : 0.6602564102564102\n",
      "Generation 306\n",
      "Best result : 0.6602564102564102\n",
      "Generation 307\n",
      "Best result : 0.6602564102564102\n",
      "Generation 308\n",
      "Best result : 0.6602564102564102\n",
      "Generation 309\n",
      "Best result : 0.6602564102564102\n",
      "Generation 310\n",
      "Best result : 0.6602564102564102\n",
      "Generation 311\n",
      "Best result : 0.6602564102564102\n",
      "Generation 312\n",
      "Best result : 0.6602564102564102\n",
      "Generation 313\n",
      "Best result : 0.6602564102564102\n",
      "Generation 314\n",
      "Best result : 0.6602564102564102\n",
      "Generation 315\n",
      "Best result : 0.6602564102564102\n",
      "Generation 316\n",
      "Best result : 0.6602564102564102\n",
      "Generation 317\n",
      "Best result : 0.6602564102564102\n",
      "Generation 318\n",
      "Best result : 0.6602564102564102\n",
      "Generation 319\n",
      "Best result : 0.6602564102564102\n",
      "Generation 320\n",
      "Best result : 0.6602564102564102\n",
      "Generation 321\n",
      "Best result : 0.6602564102564102\n",
      "Generation 322\n",
      "Best result : 0.6602564102564102\n",
      "Generation 323\n",
      "Best result : 0.6602564102564102\n",
      "Generation 324\n",
      "Best result : 0.6602564102564102\n",
      "Generation 325\n",
      "Best result : 0.6602564102564102\n",
      "Generation 326\n",
      "Best result : 0.6602564102564102\n",
      "Generation 327\n",
      "Best result : 0.6602564102564102\n",
      "Generation 328\n",
      "Best result : 0.6602564102564102\n",
      "Generation 329\n",
      "Best result : 0.6602564102564102\n",
      "Generation 330\n",
      "Best result : 0.6602564102564102\n",
      "Generation 331\n",
      "Best result : 0.6602564102564102\n",
      "Generation 332\n",
      "Best result : 0.6602564102564102\n",
      "Generation 333\n",
      "Best result : 0.6602564102564102\n",
      "Generation 334\n",
      "Best result : 0.6602564102564102\n",
      "Generation 335\n",
      "Best result : 0.6602564102564102\n",
      "Generation 336\n",
      "Best result : 0.6602564102564102\n",
      "Generation 337\n",
      "Best result : 0.6602564102564102\n",
      "Generation 338\n",
      "Best result : 0.6602564102564102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 339\n",
      "Best result : 0.6602564102564102\n",
      "Generation 340\n",
      "Best result : 0.6602564102564102\n",
      "Generation 341\n",
      "Best result : 0.6602564102564102\n",
      "Generation 342\n",
      "Best result : 0.6602564102564102\n",
      "Generation 343\n",
      "Best result : 0.6602564102564102\n",
      "Generation 344\n",
      "Best result : 0.6602564102564102\n",
      "Generation 345\n",
      "Best result : 0.6602564102564102\n",
      "Generation 346\n",
      "Best result : 0.6602564102564102\n",
      "Generation 347\n",
      "Best result : 0.6602564102564102\n",
      "Generation 348\n",
      "Best result : 0.6602564102564102\n",
      "Generation 349\n",
      "Best result : 0.6602564102564102\n",
      "Generation 350\n",
      "Best result : 0.6602564102564102\n",
      "Generation 351\n",
      "Best result : 0.6602564102564102\n",
      "Generation 352\n",
      "Best result : 0.6602564102564102\n",
      "Generation 353\n",
      "Best result : 0.6602564102564102\n",
      "Generation 354\n",
      "Best result : 0.6602564102564102\n",
      "Generation 355\n",
      "Best result : 0.6602564102564102\n",
      "Generation 356\n",
      "Best result : 0.6602564102564102\n",
      "Generation 357\n",
      "Best result : 0.6602564102564102\n",
      "Generation 358\n",
      "Best result : 0.6602564102564102\n",
      "Generation 359\n",
      "Best result : 0.6602564102564102\n",
      "Generation 360\n",
      "Best result : 0.6602564102564102\n",
      "Generation 361\n",
      "Best result : 0.6602564102564102\n",
      "Generation 362\n",
      "Best result : 0.6602564102564102\n",
      "Generation 363\n",
      "Best result : 0.6602564102564102\n",
      "Generation 364\n",
      "Best result : 0.6602564102564102\n",
      "Generation 365\n",
      "Best result : 0.6602564102564102\n",
      "Generation 366\n",
      "Best result : 0.6602564102564102\n",
      "Generation 367\n",
      "Best result : 0.6602564102564102\n",
      "Generation 368\n",
      "Best result : 0.6602564102564102\n",
      "Generation 369\n",
      "Best result : 0.6602564102564102\n",
      "Generation 370\n",
      "Best result : 0.6602564102564102\n",
      "Generation 371\n",
      "Best result : 0.6602564102564102\n",
      "Generation 372\n",
      "Best result : 0.6602564102564102\n",
      "Generation 373\n",
      "Best result : 0.6602564102564102\n",
      "Generation 374\n",
      "Best result : 0.6602564102564102\n",
      "Generation 375\n",
      "Best result : 0.6602564102564102\n",
      "Generation 376\n",
      "Best result : 0.6602564102564102\n",
      "Generation 377\n",
      "Best result : 0.6602564102564102\n",
      "Generation 378\n",
      "Best result : 0.6602564102564102\n",
      "Generation 379\n",
      "Best result : 0.6602564102564102\n",
      "Generation 380\n",
      "Best result : 0.6602564102564102\n",
      "Generation 381\n",
      "Best result : 0.6602564102564102\n",
      "Generation 382\n",
      "Best result : 0.6602564102564102\n",
      "Generation 383\n",
      "Best result : 0.6602564102564102\n",
      "Generation 384\n",
      "Best result : 0.6602564102564102\n",
      "Generation 385\n",
      "Best result : 0.6602564102564102\n",
      "Generation 386\n",
      "Best result : 0.6602564102564102\n",
      "Generation 387\n",
      "Best result : 0.6602564102564102\n",
      "Generation 388\n",
      "Best result : 0.6602564102564102\n",
      "Generation 389\n",
      "Best result : 0.6602564102564102\n",
      "Generation 390\n",
      "Best result : 0.6602564102564102\n",
      "Generation 391\n",
      "Best result : 0.6602564102564102\n",
      "Generation 392\n",
      "Best result : 0.6602564102564102\n",
      "Generation 393\n",
      "Best result : 0.6602564102564102\n",
      "Generation 394\n",
      "Best result : 0.6602564102564102\n",
      "Generation 395\n",
      "Best result : 0.6602564102564102\n",
      "Generation 396\n",
      "Best result : 0.6602564102564102\n",
      "Generation 397\n",
      "Best result : 0.6602564102564102\n",
      "Generation 398\n",
      "Best result : 0.6602564102564102\n",
      "Generation 399\n",
      "Best result : 0.6602564102564102\n",
      "Generation 400\n",
      "Best result : 0.6602564102564102\n",
      "Generation 401\n",
      "Best result : 0.6602564102564102\n",
      "Generation 402\n",
      "Best result : 0.6602564102564102\n",
      "Generation 403\n",
      "Best result : 0.6602564102564102\n",
      "Generation 404\n",
      "Best result : 0.6602564102564102\n",
      "Generation 405\n",
      "Best result : 0.6602564102564102\n",
      "Generation 406\n",
      "Best result : 0.6602564102564102\n",
      "Generation 407\n",
      "Best result : 0.6602564102564102\n",
      "Generation 408\n",
      "Best result : 0.6602564102564102\n",
      "Generation 409\n",
      "Best result : 0.6602564102564102\n",
      "Generation 410\n",
      "Best result : 0.6602564102564102\n",
      "Generation 411\n",
      "Best result : 0.6602564102564102\n",
      "Generation 412\n",
      "Best result : 0.6602564102564102\n",
      "Generation 413\n",
      "Best result : 0.6602564102564102\n",
      "Generation 414\n",
      "Best result : 0.6602564102564102\n",
      "Generation 415\n",
      "Best result : 0.6602564102564102\n",
      "Generation 416\n",
      "Best result : 0.6602564102564102\n",
      "Generation 417\n",
      "Best result : 0.6602564102564102\n",
      "Generation 418\n",
      "Best result : 0.6602564102564102\n",
      "Generation 419\n",
      "Best result : 0.6602564102564102\n",
      "Generation 420\n",
      "Best result : 0.6602564102564102\n",
      "Generation 421\n",
      "Best result : 0.6602564102564102\n",
      "Generation 422\n",
      "Best result : 0.6602564102564102\n",
      "Generation 423\n",
      "Best result : 0.6602564102564102\n",
      "Generation 424\n",
      "Best result : 0.6602564102564102\n",
      "Generation 425\n",
      "Best result : 0.6602564102564102\n",
      "Generation 426\n",
      "Best result : 0.6602564102564102\n",
      "Generation 427\n",
      "Best result : 0.6602564102564102\n",
      "Generation 428\n",
      "Best result : 0.6602564102564102\n",
      "Generation 429\n",
      "Best result : 0.6602564102564102\n",
      "Generation 430\n",
      "Best result : 0.6602564102564102\n",
      "Generation 431\n",
      "Best result : 0.6602564102564102\n",
      "Generation 432\n",
      "Best result : 0.6602564102564102\n",
      "Generation 433\n",
      "Best result : 0.6602564102564102\n",
      "Generation 434\n",
      "Best result : 0.6602564102564102\n",
      "Generation 435\n",
      "Best result : 0.6602564102564102\n",
      "Generation 436\n",
      "Best result : 0.6602564102564102\n",
      "Generation 437\n",
      "Best result : 0.6602564102564102\n",
      "Generation 438\n",
      "Best result : 0.6602564102564102\n",
      "Generation 439\n",
      "Best result : 0.6602564102564102\n",
      "Generation 440\n",
      "Best result : 0.6602564102564102\n",
      "Generation 441\n",
      "Best result : 0.6602564102564102\n",
      "Generation 442\n",
      "Best result : 0.6602564102564102\n",
      "Generation 443\n",
      "Best result : 0.6602564102564102\n",
      "Generation 444\n",
      "Best result : 0.6602564102564102\n",
      "Generation 445\n",
      "Best result : 0.6602564102564102\n",
      "Generation 446\n",
      "Best result : 0.6602564102564102\n",
      "Generation 447\n",
      "Best result : 0.6602564102564102\n",
      "Generation 448\n",
      "Best result : 0.6602564102564102\n",
      "Generation 449\n",
      "Best result : 0.6602564102564102\n",
      "Generation 450\n",
      "Best result : 0.6602564102564102\n",
      "Generation 451\n",
      "Best result : 0.6602564102564102\n",
      "Generation 452\n",
      "Best result : 0.6602564102564102\n",
      "Generation 453\n",
      "Best result : 0.6602564102564102\n",
      "Generation 454\n",
      "Best result : 0.6602564102564102\n",
      "Generation 455\n",
      "Best result : 0.6602564102564102\n",
      "Generation 456\n",
      "Best result : 0.6602564102564102\n",
      "Generation 457\n",
      "Best result : 0.6602564102564102\n",
      "Generation 458\n",
      "Best result : 0.6602564102564102\n",
      "Generation 459\n",
      "Best result : 0.6602564102564102\n",
      "Generation 460\n",
      "Best result : 0.6602564102564102\n",
      "Generation 461\n",
      "Best result : 0.6602564102564102\n",
      "Generation 462\n",
      "Best result : 0.6602564102564102\n",
      "Generation 463\n",
      "Best result : 0.6602564102564102\n",
      "Generation 464\n",
      "Best result : 0.6602564102564102\n",
      "Generation 465\n",
      "Best result : 0.6602564102564102\n",
      "Generation 466\n",
      "Best result : 0.6602564102564102\n",
      "Generation 467\n",
      "Best result : 0.6602564102564102\n",
      "Generation 468\n",
      "Best result : 0.6602564102564102\n",
      "Generation 469\n",
      "Best result : 0.6602564102564102\n",
      "Generation 470\n",
      "Best result : 0.6602564102564102\n",
      "Generation 471\n",
      "Best result : 0.6602564102564102\n",
      "Generation 472\n",
      "Best result : 0.6602564102564102\n",
      "Generation 473\n",
      "Best result : 0.6602564102564102\n",
      "Generation 474\n",
      "Best result : 0.6602564102564102\n",
      "Generation 475\n",
      "Best result : 0.6602564102564102\n",
      "Generation 476\n",
      "Best result : 0.6602564102564102\n",
      "Generation 477\n",
      "Best result : 0.6602564102564102\n",
      "Generation 478\n",
      "Best result : 0.6602564102564102\n",
      "Generation 479\n",
      "Best result : 0.6602564102564102\n",
      "Generation 480\n",
      "Best result : 0.6602564102564102\n",
      "Generation 481\n",
      "Best result : 0.6602564102564102\n",
      "Generation 482\n",
      "Best result : 0.6602564102564102\n",
      "Generation 483\n",
      "Best result : 0.6602564102564102\n",
      "Generation 484\n",
      "Best result : 0.6602564102564102\n",
      "Generation 485\n",
      "Best result : 0.6602564102564102\n",
      "Generation 486\n",
      "Best result : 0.6602564102564102\n",
      "Generation 487\n",
      "Best result : 0.6602564102564102\n",
      "Generation 488\n",
      "Best result : 0.6602564102564102\n",
      "Generation 489\n",
      "Best result : 0.6602564102564102\n",
      "Generation 490\n",
      "Best result : 0.6602564102564102\n",
      "Generation 491\n",
      "Best result : 0.6602564102564102\n",
      "Generation 492\n",
      "Best result : 0.6602564102564102\n",
      "Generation 493\n",
      "Best result : 0.6602564102564102\n",
      "Generation 494\n",
      "Best result : 0.6602564102564102\n",
      "Generation 495\n",
      "Best result : 0.6602564102564102\n",
      "Generation 496\n",
      "Best result : 0.6602564102564102\n",
      "Generation 497\n",
      "Best result : 0.6602564102564102\n",
      "Generation 498\n",
      "Best result : 0.6602564102564102\n",
      "Generation 499\n",
      "Best result : 0.6602564102564102\n",
      "Generation 500\n",
      "Best result : 0.6602564102564102\n",
      "Generation 501\n",
      "Best result : 0.6602564102564102\n",
      "Generation 502\n",
      "Best result : 0.6602564102564102\n",
      "Generation 503\n",
      "Best result : 0.6602564102564102\n",
      "Generation 504\n",
      "Best result : 0.6602564102564102\n",
      "Generation 505\n",
      "Best result : 0.6602564102564102\n",
      "Generation 506\n",
      "Best result : 0.6602564102564102\n",
      "Generation 507\n",
      "Best result : 0.6602564102564102\n",
      "Generation 508\n",
      "Best result : 0.6602564102564102\n",
      "Generation 509\n",
      "Best result : 0.6602564102564102\n",
      "Generation 510\n",
      "Best result : 0.6602564102564102\n",
      "Generation 511\n",
      "Best result : 0.6602564102564102\n",
      "Generation 512\n",
      "Best result : 0.6602564102564102\n",
      "Generation 513\n",
      "Best result : 0.6602564102564102\n",
      "Generation 514\n",
      "Best result : 0.6602564102564102\n",
      "Generation 515\n",
      "Best result : 0.6602564102564102\n",
      "Generation 516\n",
      "Best result : 0.6602564102564102\n",
      "Generation 517\n",
      "Best result : 0.6602564102564102\n",
      "Generation 518\n",
      "Best result : 0.6602564102564102\n",
      "Generation 519\n",
      "Best result : 0.6602564102564102\n",
      "Generation 520\n",
      "Best result : 0.6602564102564102\n",
      "Generation 521\n",
      "Best result : 0.6602564102564102\n",
      "Generation 522\n",
      "Best result : 0.6602564102564102\n",
      "Generation 523\n",
      "Best result : 0.6602564102564102\n",
      "Generation 524\n",
      "Best result : 0.6602564102564102\n",
      "Generation 525\n",
      "Best result : 0.6602564102564102\n",
      "Generation 526\n",
      "Best result : 0.6602564102564102\n",
      "Generation 527\n",
      "Best result : 0.6602564102564102\n",
      "Generation 528\n",
      "Best result : 0.6602564102564102\n",
      "Generation 529\n",
      "Best result : 0.6602564102564102\n",
      "Generation 530\n",
      "Best result : 0.6602564102564102\n",
      "Generation 531\n",
      "Best result : 0.6602564102564102\n",
      "Generation 532\n",
      "Best result : 0.6602564102564102\n",
      "Generation 533\n",
      "Best result : 0.6602564102564102\n",
      "Generation 534\n",
      "Best result : 0.6602564102564102\n",
      "Generation 535\n",
      "Best result : 0.6602564102564102\n",
      "Generation 536\n",
      "Best result : 0.6602564102564102\n",
      "Generation 537\n",
      "Best result : 0.6602564102564102\n",
      "Generation 538\n",
      "Best result : 0.6602564102564102\n",
      "Generation 539\n",
      "Best result : 0.6602564102564102\n",
      "Generation 540\n",
      "Best result : 0.6602564102564102\n",
      "Generation 541\n",
      "Best result : 0.6602564102564102\n",
      "Generation 542\n",
      "Best result : 0.6602564102564102\n",
      "Generation 543\n",
      "Best result : 0.6602564102564102\n",
      "Generation 544\n",
      "Best result : 0.6602564102564102\n",
      "Generation 545\n",
      "Best result : 0.6602564102564102\n",
      "Generation 546\n",
      "Best result : 0.6602564102564102\n",
      "Generation 547\n",
      "Best result : 0.6602564102564102\n",
      "Generation 548\n",
      "Best result : 0.6602564102564102\n",
      "Generation 549\n",
      "Best result : 0.6602564102564102\n",
      "Generation 550\n",
      "Best result : 0.6602564102564102\n",
      "Generation 551\n",
      "Best result : 0.6602564102564102\n",
      "Generation 552\n",
      "Best result : 0.6602564102564102\n",
      "Generation 553\n",
      "Best result : 0.6602564102564102\n",
      "Generation 554\n",
      "Best result : 0.6602564102564102\n",
      "Generation 555\n",
      "Best result : 0.6602564102564102\n",
      "Generation 556\n",
      "Best result : 0.6602564102564102\n",
      "Generation 557\n",
      "Best result : 0.6602564102564102\n",
      "Generation 558\n",
      "Best result : 0.6602564102564102\n",
      "Generation 559\n",
      "Best result : 0.6602564102564102\n",
      "Generation 560\n",
      "Best result : 0.6602564102564102\n",
      "Generation 561\n",
      "Best result : 0.6602564102564102\n",
      "Generation 562\n",
      "Best result : 0.6602564102564102\n",
      "Generation 563\n",
      "Best result : 0.6602564102564102\n",
      "Generation 564\n",
      "Best result : 0.6602564102564102\n",
      "Generation 565\n",
      "Best result : 0.6602564102564102\n",
      "Generation 566\n",
      "Best result : 0.6602564102564102\n",
      "Generation 567\n",
      "Best result : 0.6602564102564102\n",
      "Generation 568\n",
      "Best result : 0.6602564102564102\n",
      "Generation 569\n",
      "Best result : 0.6602564102564102\n",
      "Generation 570\n",
      "Best result : 0.6602564102564102\n",
      "Generation 571\n",
      "Best result : 0.6602564102564102\n",
      "Generation 572\n",
      "Best result : 0.6602564102564102\n",
      "Generation 573\n",
      "Best result : 0.6602564102564102\n",
      "Generation 574\n",
      "Best result : 0.6602564102564102\n",
      "Generation 575\n",
      "Best result : 0.6602564102564102\n",
      "Generation 576\n",
      "Best result : 0.6602564102564102\n",
      "Generation 577\n",
      "Best result : 0.6602564102564102\n",
      "Generation 578\n",
      "Best result : 0.6602564102564102\n",
      "Generation 579\n",
      "Best result : 0.6602564102564102\n",
      "Generation 580\n",
      "Best result : 0.6602564102564102\n",
      "Generation 581\n",
      "Best result : 0.6602564102564102\n",
      "Generation 582\n",
      "Best result : 0.6602564102564102\n",
      "Generation 583\n",
      "Best result : 0.6602564102564102\n",
      "Generation 584\n",
      "Best result : 0.6602564102564102\n",
      "Generation 585\n",
      "Best result : 0.6602564102564102\n",
      "Generation 586\n",
      "Best result : 0.6602564102564102\n",
      "Generation 587\n",
      "Best result : 0.6602564102564102\n",
      "Generation 588\n",
      "Best result : 0.6602564102564102\n",
      "Generation 589\n",
      "Best result : 0.6602564102564102\n",
      "Generation 590\n",
      "Best result : 0.6602564102564102\n",
      "Generation 591\n",
      "Best result : 0.6602564102564102\n",
      "Generation 592\n",
      "Best result : 0.6602564102564102\n",
      "Generation 593\n",
      "Best result : 0.6602564102564102\n",
      "Generation 594\n",
      "Best result : 0.6602564102564102\n",
      "Generation 595\n",
      "Best result : 0.6602564102564102\n",
      "Generation 596\n",
      "Best result : 0.6602564102564102\n",
      "Generation 597\n",
      "Best result : 0.6602564102564102\n",
      "Generation 598\n",
      "Best result : 0.6602564102564102\n",
      "Generation 599\n",
      "Best result : 0.6602564102564102\n",
      "Generation 600\n",
      "Best result : 0.6602564102564102\n",
      "Generation 601\n",
      "Best result : 0.6602564102564102\n",
      "Generation 602\n",
      "Best result : 0.6602564102564102\n",
      "Generation 603\n",
      "Best result : 0.6602564102564102\n",
      "Generation 604\n",
      "Best result : 0.6602564102564102\n",
      "Generation 605\n",
      "Best result : 0.6602564102564102\n",
      "Generation 606\n",
      "Best result : 0.6602564102564102\n",
      "Generation 607\n",
      "Best result : 0.6602564102564102\n",
      "Generation 608\n",
      "Best result : 0.6602564102564102\n",
      "Generation 609\n",
      "Best result : 0.6602564102564102\n",
      "Generation 610\n",
      "Best result : 0.6602564102564102\n",
      "Generation 611\n",
      "Best result : 0.6602564102564102\n",
      "Generation 612\n",
      "Best result : 0.6602564102564102\n",
      "Generation 613\n",
      "Best result : 0.6602564102564102\n",
      "Generation 614\n",
      "Best result : 0.6602564102564102\n",
      "Generation 615\n",
      "Best result : 0.6602564102564102\n",
      "Generation 616\n",
      "Best result : 0.6602564102564102\n",
      "Generation 617\n",
      "Best result : 0.6602564102564102\n",
      "Generation 618\n",
      "Best result : 0.6602564102564102\n",
      "Generation 619\n",
      "Best result : 0.6602564102564102\n",
      "Generation 620\n",
      "Best result : 0.6602564102564102\n",
      "Generation 621\n",
      "Best result : 0.6602564102564102\n",
      "Generation 622\n",
      "Best result : 0.6602564102564102\n",
      "Generation 623\n",
      "Best result : 0.6602564102564102\n",
      "Generation 624\n",
      "Best result : 0.6602564102564102\n",
      "Generation 625\n",
      "Best result : 0.6602564102564102\n",
      "Generation 626\n",
      "Best result : 0.6602564102564102\n",
      "Generation 627\n",
      "Best result : 0.6602564102564102\n",
      "Generation 628\n",
      "Best result : 0.6602564102564102\n",
      "Generation 629\n",
      "Best result : 0.6602564102564102\n",
      "Generation 630\n",
      "Best result : 0.6602564102564102\n",
      "Generation 631\n",
      "Best result : 0.6602564102564102\n",
      "Generation 632\n",
      "Best result : 0.6602564102564102\n",
      "Generation 633\n",
      "Best result : 0.6602564102564102\n",
      "Generation 634\n",
      "Best result : 0.6602564102564102\n",
      "Generation 635\n",
      "Best result : 0.6602564102564102\n",
      "Generation 636\n",
      "Best result : 0.6602564102564102\n",
      "Generation 637\n",
      "Best result : 0.6602564102564102\n",
      "Generation 638\n",
      "Best result : 0.6602564102564102\n",
      "Generation 639\n",
      "Best result : 0.6602564102564102\n",
      "Generation 640\n",
      "Best result : 0.6602564102564102\n",
      "Generation 641\n",
      "Best result : 0.6602564102564102\n",
      "Generation 642\n",
      "Best result : 0.6602564102564102\n",
      "Generation 643\n",
      "Best result : 0.6602564102564102\n",
      "Generation 644\n",
      "Best result : 0.6602564102564102\n",
      "Generation 645\n",
      "Best result : 0.6602564102564102\n",
      "Generation 646\n",
      "Best result : 0.6602564102564102\n",
      "Generation 647\n",
      "Best result : 0.6602564102564102\n",
      "Generation 648\n",
      "Best result : 0.6602564102564102\n",
      "Generation 649\n",
      "Best result : 0.6602564102564102\n",
      "Generation 650\n",
      "Best result : 0.6602564102564102\n",
      "Generation 651\n",
      "Best result : 0.6602564102564102\n",
      "Generation 652\n",
      "Best result : 0.6602564102564102\n",
      "Generation 653\n",
      "Best result : 0.6602564102564102\n",
      "Generation 654\n",
      "Best result : 0.6602564102564102\n",
      "Generation 655\n",
      "Best result : 0.6602564102564102\n",
      "Generation 656\n",
      "Best result : 0.6602564102564102\n",
      "Generation 657\n",
      "Best result : 0.6602564102564102\n",
      "Generation 658\n",
      "Best result : 0.6602564102564102\n",
      "Generation 659\n",
      "Best result : 0.6602564102564102\n",
      "Generation 660\n",
      "Best result : 0.6602564102564102\n",
      "Generation 661\n",
      "Best result : 0.6602564102564102\n",
      "Generation 662\n",
      "Best result : 0.6602564102564102\n",
      "Generation 663\n",
      "Best result : 0.6602564102564102\n",
      "Generation 664\n",
      "Best result : 0.6602564102564102\n",
      "Generation 665\n",
      "Best result : 0.6602564102564102\n",
      "Generation 666\n",
      "Best result : 0.6602564102564102\n",
      "Generation 667\n",
      "Best result : 0.6602564102564102\n",
      "Generation 668\n",
      "Best result : 0.6602564102564102\n",
      "Generation 669\n",
      "Best result : 0.6602564102564102\n",
      "Generation 670\n",
      "Best result : 0.6602564102564102\n",
      "Generation 671\n",
      "Best result : 0.6602564102564102\n",
      "Generation 672\n",
      "Best result : 0.6602564102564102\n",
      "Generation 673\n",
      "Best result : 0.6602564102564102\n",
      "Generation 674\n",
      "Best result : 0.6602564102564102\n",
      "Generation 675\n",
      "Best result : 0.6602564102564102\n",
      "Generation 676\n",
      "Best result : 0.6602564102564102\n",
      "Generation 677\n",
      "Best result : 0.6602564102564102\n",
      "Generation 678\n",
      "Best result : 0.6602564102564102\n",
      "Generation 679\n",
      "Best result : 0.6602564102564102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 680\n",
      "Best result : 0.6602564102564102\n",
      "Generation 681\n",
      "Best result : 0.6602564102564102\n",
      "Generation 682\n",
      "Best result : 0.6602564102564102\n",
      "Generation 683\n",
      "Best result : 0.6602564102564102\n",
      "Generation 684\n",
      "Best result : 0.6602564102564102\n",
      "Generation 685\n",
      "Best result : 0.6602564102564102\n",
      "Generation 686\n",
      "Best result : 0.6602564102564102\n",
      "Generation 687\n",
      "Best result : 0.6602564102564102\n",
      "Generation 688\n",
      "Best result : 0.6602564102564102\n",
      "Generation 689\n",
      "Best result : 0.6602564102564102\n",
      "Generation 690\n",
      "Best result : 0.6602564102564102\n",
      "Generation 691\n",
      "Best result : 0.6602564102564102\n",
      "Generation 692\n",
      "Best result : 0.6602564102564102\n",
      "Generation 693\n",
      "Best result : 0.6602564102564102\n",
      "Generation 694\n",
      "Best result : 0.6602564102564102\n",
      "Generation 695\n",
      "Best result : 0.6602564102564102\n",
      "Generation 696\n",
      "Best result : 0.6602564102564102\n",
      "Generation 697\n",
      "Best result : 0.6602564102564102\n",
      "Generation 698\n",
      "Best result : 0.6602564102564102\n",
      "Generation 699\n",
      "Best result : 0.6602564102564102\n",
      "Generation 700\n",
      "Best result : 0.6602564102564102\n",
      "Generation 701\n",
      "Best result : 0.6602564102564102\n",
      "Generation 702\n",
      "Best result : 0.6602564102564102\n",
      "Generation 703\n",
      "Best result : 0.6602564102564102\n",
      "Generation 704\n",
      "Best result : 0.6602564102564102\n",
      "Generation 705\n",
      "Best result : 0.6602564102564102\n",
      "Generation 706\n",
      "Best result : 0.6602564102564102\n",
      "Generation 707\n",
      "Best result : 0.6602564102564102\n",
      "Generation 708\n",
      "Best result : 0.6602564102564102\n",
      "Generation 709\n",
      "Best result : 0.6602564102564102\n",
      "Generation 710\n",
      "Best result : 0.6602564102564102\n",
      "Generation 711\n",
      "Best result : 0.6602564102564102\n",
      "Generation 712\n",
      "Best result : 0.6602564102564102\n",
      "Generation 713\n",
      "Best result : 0.6602564102564102\n",
      "Generation 714\n",
      "Best result : 0.6602564102564102\n",
      "Generation 715\n",
      "Best result : 0.6602564102564102\n",
      "Generation 716\n",
      "Best result : 0.6602564102564102\n",
      "Generation 717\n",
      "Best result : 0.6602564102564102\n",
      "Generation 718\n",
      "Best result : 0.6602564102564102\n",
      "Generation 719\n",
      "Best result : 0.6602564102564102\n",
      "Generation 720\n",
      "Best result : 0.6602564102564102\n",
      "Generation 721\n",
      "Best result : 0.6602564102564102\n",
      "Generation 722\n",
      "Best result : 0.6602564102564102\n",
      "Generation 723\n",
      "Best result : 0.6602564102564102\n",
      "Generation 724\n",
      "Best result : 0.6602564102564102\n",
      "Generation 725\n",
      "Best result : 0.6602564102564102\n",
      "Generation 726\n",
      "Best result : 0.6602564102564102\n",
      "Generation 727\n",
      "Best result : 0.6602564102564102\n",
      "Generation 728\n",
      "Best result : 0.6602564102564102\n",
      "Generation 729\n",
      "Best result : 0.6602564102564102\n",
      "Generation 730\n",
      "Best result : 0.6602564102564102\n",
      "Generation 731\n",
      "Best result : 0.6602564102564102\n",
      "Generation 732\n",
      "Best result : 0.6602564102564102\n",
      "Generation 733\n",
      "Best result : 0.6602564102564102\n",
      "Generation 734\n",
      "Best result : 0.6602564102564102\n",
      "Generation 735\n",
      "Best result : 0.6602564102564102\n",
      "Generation 736\n",
      "Best result : 0.6602564102564102\n",
      "Generation 737\n",
      "Best result : 0.6602564102564102\n",
      "Generation 738\n",
      "Best result : 0.6602564102564102\n",
      "Generation 739\n",
      "Best result : 0.6602564102564102\n",
      "Generation 740\n",
      "Best result : 0.6602564102564102\n",
      "Generation 741\n",
      "Best result : 0.6602564102564102\n",
      "Generation 742\n",
      "Best result : 0.6602564102564102\n",
      "Generation 743\n",
      "Best result : 0.6602564102564102\n",
      "Generation 744\n",
      "Best result : 0.6602564102564102\n",
      "Generation 745\n",
      "Best result : 0.6602564102564102\n",
      "Generation 746\n",
      "Best result : 0.6602564102564102\n",
      "Generation 747\n",
      "Best result : 0.6602564102564102\n",
      "Generation 748\n",
      "Best result : 0.6602564102564102\n",
      "Generation 749\n",
      "Best result : 0.6602564102564102\n",
      "Generation 750\n",
      "Best result : 0.6602564102564102\n",
      "Generation 751\n",
      "Best result : 0.6602564102564102\n",
      "Generation 752\n",
      "Best result : 0.6602564102564102\n",
      "Generation 753\n",
      "Best result : 0.6602564102564102\n",
      "Generation 754\n",
      "Best result : 0.6602564102564102\n",
      "Generation 755\n",
      "Best result : 0.6602564102564102\n",
      "Generation 756\n",
      "Best result : 0.6602564102564102\n",
      "Generation 757\n",
      "Best result : 0.6602564102564102\n",
      "Generation 758\n",
      "Best result : 0.6602564102564102\n",
      "Generation 759\n",
      "Best result : 0.6602564102564102\n",
      "Generation 760\n",
      "Best result : 0.6602564102564102\n",
      "Generation 761\n",
      "Best result : 0.6602564102564102\n",
      "Generation 762\n",
      "Best result : 0.6602564102564102\n",
      "Generation 763\n",
      "Best result : 0.6602564102564102\n",
      "Generation 764\n",
      "Best result : 0.6602564102564102\n",
      "Generation 765\n",
      "Best result : 0.6602564102564102\n",
      "Generation 766\n",
      "Best result : 0.6602564102564102\n",
      "Generation 767\n",
      "Best result : 0.6602564102564102\n",
      "Generation 768\n",
      "Best result : 0.6602564102564102\n",
      "Generation 769\n",
      "Best result : 0.6602564102564102\n",
      "Generation 770\n",
      "Best result : 0.6602564102564102\n",
      "Generation 771\n",
      "Best result : 0.6602564102564102\n",
      "Generation 772\n",
      "Best result : 0.6602564102564102\n",
      "Generation 773\n",
      "Best result : 0.6602564102564102\n",
      "Generation 774\n",
      "Best result : 0.6602564102564102\n",
      "Generation 775\n",
      "Best result : 0.6602564102564102\n",
      "Generation 776\n",
      "Best result : 0.6602564102564102\n",
      "Generation 777\n",
      "Best result : 0.6602564102564102\n",
      "Generation 778\n",
      "Best result : 0.6602564102564102\n",
      "Generation 779\n",
      "Best result : 0.6602564102564102\n",
      "Generation 780\n",
      "Best result : 0.6602564102564102\n",
      "Generation 781\n",
      "Best result : 0.6602564102564102\n",
      "Generation 782\n",
      "Best result : 0.6602564102564102\n",
      "Generation 783\n",
      "Best result : 0.6602564102564102\n",
      "Generation 784\n",
      "Best result : 0.6602564102564102\n",
      "Generation 785\n",
      "Best result : 0.6602564102564102\n",
      "Generation 786\n",
      "Best result : 0.6602564102564102\n",
      "Generation 787\n",
      "Best result : 0.6602564102564102\n",
      "Generation 788\n",
      "Best result : 0.6602564102564102\n",
      "Generation 789\n",
      "Best result : 0.6602564102564102\n",
      "Generation 790\n",
      "Best result : 0.6602564102564102\n",
      "Generation 791\n",
      "Best result : 0.6602564102564102\n",
      "Generation 792\n",
      "Best result : 0.6602564102564102\n",
      "Generation 793\n",
      "Best result : 0.6602564102564102\n",
      "Generation 794\n",
      "Best result : 0.6602564102564102\n",
      "Generation 795\n",
      "Best result : 0.6602564102564102\n",
      "Generation 796\n",
      "Best result : 0.6602564102564102\n",
      "Generation 797\n",
      "Best result : 0.6602564102564102\n",
      "Generation 798\n",
      "Best result : 0.6602564102564102\n",
      "Generation 799\n",
      "Best result : 0.6602564102564102\n",
      "Generation 800\n",
      "Best result : 0.6602564102564102\n",
      "Generation 801\n",
      "Best result : 0.6602564102564102\n",
      "Generation 802\n",
      "Best result : 0.6602564102564102\n",
      "Generation 803\n",
      "Best result : 0.6602564102564102\n",
      "Generation 804\n",
      "Best result : 0.6602564102564102\n",
      "Generation 805\n",
      "Best result : 0.6602564102564102\n",
      "Generation 806\n",
      "Best result : 0.6602564102564102\n",
      "Generation 807\n",
      "Best result : 0.6602564102564102\n",
      "Generation 808\n",
      "Best result : 0.6602564102564102\n",
      "Generation 809\n",
      "Best result : 0.6602564102564102\n",
      "Generation 810\n",
      "Best result : 0.6602564102564102\n",
      "Generation 811\n",
      "Best result : 0.6602564102564102\n",
      "Generation 812\n",
      "Best result : 0.6602564102564102\n",
      "Generation 813\n",
      "Best result : 0.6602564102564102\n",
      "Generation 814\n",
      "Best result : 0.6602564102564102\n",
      "Generation 815\n",
      "Best result : 0.6602564102564102\n",
      "Generation 816\n",
      "Best result : 0.6602564102564102\n",
      "Generation 817\n",
      "Best result : 0.6602564102564102\n",
      "Generation 818\n",
      "Best result : 0.6602564102564102\n",
      "Generation 819\n",
      "Best result : 0.6602564102564102\n",
      "Generation 820\n",
      "Best result : 0.6602564102564102\n",
      "Generation 821\n",
      "Best result : 0.6602564102564102\n",
      "Generation 822\n",
      "Best result : 0.6602564102564102\n",
      "Generation 823\n",
      "Best result : 0.6602564102564102\n",
      "Generation 824\n",
      "Best result : 0.6602564102564102\n",
      "Generation 825\n",
      "Best result : 0.6602564102564102\n",
      "Generation 826\n",
      "Best result : 0.6602564102564102\n",
      "Generation 827\n",
      "Best result : 0.6602564102564102\n",
      "Generation 828\n",
      "Best result : 0.6602564102564102\n",
      "Generation 829\n",
      "Best result : 0.6602564102564102\n",
      "Generation 830\n",
      "Best result : 0.6602564102564102\n",
      "Generation 831\n",
      "Best result : 0.6602564102564102\n",
      "Generation 832\n",
      "Best result : 0.6602564102564102\n",
      "Generation 833\n",
      "Best result : 0.6602564102564102\n",
      "Generation 834\n",
      "Best result : 0.6602564102564102\n",
      "Generation 835\n",
      "Best result : 0.6602564102564102\n",
      "Generation 836\n",
      "Best result : 0.6602564102564102\n",
      "Generation 837\n",
      "Best result : 0.6602564102564102\n",
      "Generation 838\n",
      "Best result : 0.6602564102564102\n",
      "Generation 839\n",
      "Best result : 0.6602564102564102\n",
      "Generation 840\n",
      "Best result : 0.6602564102564102\n",
      "Generation 841\n",
      "Best result : 0.6602564102564102\n",
      "Generation 842\n",
      "Best result : 0.6602564102564102\n",
      "Generation 843\n",
      "Best result : 0.6602564102564102\n",
      "Generation 844\n",
      "Best result : 0.6602564102564102\n",
      "Generation 845\n",
      "Best result : 0.6602564102564102\n",
      "Generation 846\n",
      "Best result : 0.6602564102564102\n",
      "Generation 847\n",
      "Best result : 0.6602564102564102\n",
      "Generation 848\n",
      "Best result : 0.6602564102564102\n",
      "Generation 849\n",
      "Best result : 0.6602564102564102\n",
      "Generation 850\n",
      "Best result : 0.6602564102564102\n",
      "Generation 851\n",
      "Best result : 0.6602564102564102\n",
      "Generation 852\n",
      "Best result : 0.6602564102564102\n",
      "Generation 853\n",
      "Best result : 0.6602564102564102\n",
      "Generation 854\n",
      "Best result : 0.6602564102564102\n",
      "Generation 855\n",
      "Best result : 0.6602564102564102\n",
      "Generation 856\n",
      "Best result : 0.6602564102564102\n",
      "Generation 857\n",
      "Best result : 0.6602564102564102\n",
      "Generation 858\n",
      "Best result : 0.6602564102564102\n",
      "Generation 859\n",
      "Best result : 0.6602564102564102\n",
      "Generation 860\n",
      "Best result : 0.6602564102564102\n",
      "Generation 861\n",
      "Best result : 0.6602564102564102\n",
      "Generation 862\n",
      "Best result : 0.6602564102564102\n",
      "Generation 863\n",
      "Best result : 0.6602564102564102\n",
      "Generation 864\n",
      "Best result : 0.6602564102564102\n",
      "Generation 865\n",
      "Best result : 0.6602564102564102\n",
      "Generation 866\n",
      "Best result : 0.6602564102564102\n",
      "Generation 867\n",
      "Best result : 0.6602564102564102\n",
      "Generation 868\n",
      "Best result : 0.6602564102564102\n",
      "Generation 869\n",
      "Best result : 0.6602564102564102\n",
      "Generation 870\n",
      "Best result : 0.6602564102564102\n",
      "Generation 871\n",
      "Best result : 0.6602564102564102\n",
      "Generation 872\n",
      "Best result : 0.6602564102564102\n",
      "Generation 873\n",
      "Best result : 0.6602564102564102\n",
      "Generation 874\n",
      "Best result : 0.6602564102564102\n",
      "Generation 875\n",
      "Best result : 0.6602564102564102\n",
      "Generation 876\n",
      "Best result : 0.6602564102564102\n",
      "Generation 877\n",
      "Best result : 0.6602564102564102\n",
      "Generation 878\n",
      "Best result : 0.6602564102564102\n",
      "Generation 879\n",
      "Best result : 0.6602564102564102\n",
      "Generation 880\n",
      "Best result : 0.6602564102564102\n",
      "Generation 881\n",
      "Best result : 0.6602564102564102\n",
      "Generation 882\n",
      "Best result : 0.6602564102564102\n",
      "Generation 883\n",
      "Best result : 0.6602564102564102\n",
      "Generation 884\n",
      "Best result : 0.6602564102564102\n",
      "Generation 885\n",
      "Best result : 0.6602564102564102\n",
      "Generation 886\n",
      "Best result : 0.6602564102564102\n",
      "Generation 887\n",
      "Best result : 0.6602564102564102\n",
      "Generation 888\n",
      "Best result : 0.6602564102564102\n",
      "Generation 889\n",
      "Best result : 0.6602564102564102\n",
      "Generation 890\n",
      "Best result : 0.6602564102564102\n",
      "Generation 891\n",
      "Best result : 0.6602564102564102\n",
      "Generation 892\n",
      "Best result : 0.6602564102564102\n",
      "Generation 893\n",
      "Best result : 0.6602564102564102\n",
      "Generation 894\n",
      "Best result : 0.6602564102564102\n",
      "Generation 895\n",
      "Best result : 0.6602564102564102\n",
      "Generation 896\n",
      "Best result : 0.6602564102564102\n",
      "Generation 897\n",
      "Best result : 0.6602564102564102\n",
      "Generation 898\n",
      "Best result : 0.6602564102564102\n",
      "Generation 899\n",
      "Best result : 0.6602564102564102\n",
      "Generation 900\n",
      "Best result : 0.6602564102564102\n",
      "Generation 901\n",
      "Best result : 0.6602564102564102\n",
      "Generation 902\n",
      "Best result : 0.6602564102564102\n",
      "Generation 903\n",
      "Best result : 0.6602564102564102\n",
      "Generation 904\n",
      "Best result : 0.6602564102564102\n",
      "Generation 905\n",
      "Best result : 0.6602564102564102\n",
      "Generation 906\n",
      "Best result : 0.6602564102564102\n",
      "Generation 907\n",
      "Best result : 0.6602564102564102\n",
      "Generation 908\n",
      "Best result : 0.6602564102564102\n",
      "Generation 909\n",
      "Best result : 0.6602564102564102\n",
      "Generation 910\n",
      "Best result : 0.6602564102564102\n",
      "Generation 911\n",
      "Best result : 0.6602564102564102\n",
      "Generation 912\n",
      "Best result : 0.6602564102564102\n",
      "Generation 913\n",
      "Best result : 0.6602564102564102\n",
      "Generation 914\n",
      "Best result : 0.6602564102564102\n",
      "Generation 915\n",
      "Best result : 0.6602564102564102\n",
      "Generation 916\n",
      "Best result : 0.6602564102564102\n",
      "Generation 917\n",
      "Best result : 0.6602564102564102\n",
      "Generation 918\n",
      "Best result : 0.6602564102564102\n",
      "Generation 919\n",
      "Best result : 0.6602564102564102\n",
      "Generation 920\n",
      "Best result : 0.6602564102564102\n",
      "Generation 921\n",
      "Best result : 0.6602564102564102\n",
      "Generation 922\n",
      "Best result : 0.6602564102564102\n",
      "Generation 923\n",
      "Best result : 0.6602564102564102\n",
      "Generation 924\n",
      "Best result : 0.6602564102564102\n",
      "Generation 925\n",
      "Best result : 0.6602564102564102\n",
      "Generation 926\n",
      "Best result : 0.6602564102564102\n",
      "Generation 927\n",
      "Best result : 0.6602564102564102\n",
      "Generation 928\n",
      "Best result : 0.6602564102564102\n",
      "Generation 929\n",
      "Best result : 0.6602564102564102\n",
      "Generation 930\n",
      "Best result : 0.6602564102564102\n",
      "Generation 931\n",
      "Best result : 0.6602564102564102\n",
      "Generation 932\n",
      "Best result : 0.6602564102564102\n",
      "Generation 933\n",
      "Best result : 0.6602564102564102\n",
      "Generation 934\n",
      "Best result : 0.6602564102564102\n",
      "Generation 935\n",
      "Best result : 0.6602564102564102\n",
      "Generation 936\n",
      "Best result : 0.6602564102564102\n",
      "Generation 937\n",
      "Best result : 0.6602564102564102\n",
      "Generation 938\n",
      "Best result : 0.6602564102564102\n",
      "Generation 939\n",
      "Best result : 0.6602564102564102\n",
      "Generation 940\n",
      "Best result : 0.6602564102564102\n",
      "Generation 941\n",
      "Best result : 0.6602564102564102\n",
      "Generation 942\n",
      "Best result : 0.6602564102564102\n",
      "Generation 943\n",
      "Best result : 0.6602564102564102\n",
      "Generation 944\n",
      "Best result : 0.6602564102564102\n",
      "Generation 945\n",
      "Best result : 0.6602564102564102\n",
      "Generation 946\n",
      "Best result : 0.6602564102564102\n",
      "Generation 947\n",
      "Best result : 0.6602564102564102\n",
      "Generation 948\n",
      "Best result : 0.6602564102564102\n",
      "Generation 949\n",
      "Best result : 0.6602564102564102\n",
      "Generation 950\n",
      "Best result : 0.6602564102564102\n",
      "Generation 951\n",
      "Best result : 0.6602564102564102\n",
      "Generation 952\n",
      "Best result : 0.6602564102564102\n",
      "Generation 953\n",
      "Best result : 0.6602564102564102\n",
      "Generation 954\n",
      "Best result : 0.6602564102564102\n",
      "Generation 955\n",
      "Best result : 0.6602564102564102\n",
      "Generation 956\n",
      "Best result : 0.6602564102564102\n",
      "Generation 957\n",
      "Best result : 0.6602564102564102\n",
      "Generation 958\n",
      "Best result : 0.6602564102564102\n",
      "Generation 959\n",
      "Best result : 0.6602564102564102\n",
      "Generation 960\n",
      "Best result : 0.6602564102564102\n",
      "Generation 961\n",
      "Best result : 0.6602564102564102\n",
      "Generation 962\n",
      "Best result : 0.6602564102564102\n",
      "Generation 963\n",
      "Best result : 0.6602564102564102\n",
      "Generation 964\n",
      "Best result : 0.6602564102564102\n",
      "Generation 965\n",
      "Best result : 0.6602564102564102\n",
      "Generation 966\n",
      "Best result : 0.6602564102564102\n",
      "Generation 967\n",
      "Best result : 0.6602564102564102\n",
      "Generation 968\n",
      "Best result : 0.6602564102564102\n",
      "Generation 969\n",
      "Best result : 0.6602564102564102\n",
      "Generation 970\n",
      "Best result : 0.6602564102564102\n",
      "Generation 971\n",
      "Best result : 0.6602564102564102\n",
      "Generation 972\n",
      "Best result : 0.6602564102564102\n",
      "Generation 973\n",
      "Best result : 0.6602564102564102\n",
      "Generation 974\n",
      "Best result : 0.6602564102564102\n",
      "Generation 975\n",
      "Best result : 0.6602564102564102\n",
      "Generation 976\n",
      "Best result : 0.6602564102564102\n",
      "Generation 977\n",
      "Best result : 0.6602564102564102\n",
      "Generation 978\n",
      "Best result : 0.6602564102564102\n",
      "Generation 979\n",
      "Best result : 0.6602564102564102\n",
      "Generation 980\n",
      "Best result : 0.6602564102564102\n",
      "Generation 981\n",
      "Best result : 0.6602564102564102\n",
      "Generation 982\n",
      "Best result : 0.6602564102564102\n",
      "Generation 983\n",
      "Best result : 0.6602564102564102\n",
      "Generation 984\n",
      "Best result : 0.6602564102564102\n",
      "Generation 985\n",
      "Best result : 0.6602564102564102\n",
      "Generation 986\n",
      "Best result : 0.6602564102564102\n",
      "Generation 987\n",
      "Best result : 0.6602564102564102\n",
      "Generation 988\n",
      "Best result : 0.6602564102564102\n",
      "Generation 989\n",
      "Best result : 0.6602564102564102\n",
      "Generation 990\n",
      "Best result : 0.6602564102564102\n",
      "Generation 991\n",
      "Best result : 0.6602564102564102\n",
      "Generation 992\n",
      "Best result : 0.6602564102564102\n",
      "Generation 993\n",
      "Best result : 0.6602564102564102\n",
      "Generation 994\n",
      "Best result : 0.6602564102564102\n",
      "Generation 995\n",
      "Best result : 0.6602564102564102\n",
      "Generation 996\n",
      "Best result : 0.6602564102564102\n",
      "Generation 997\n",
      "Best result : 0.6602564102564102\n",
      "Generation 998\n",
      "Best result : 0.6602564102564102\n",
      "Generation 999\n",
      "Best result : 0.6602564102564102\n",
      "Final AUC 0.6602564102564102\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[142820,\n",
       " 121728,\n",
       " 103720,\n",
       " 93024,\n",
       " 89831,\n",
       " 34655,\n",
       " 124289,\n",
       " 60038,\n",
       " 8481,\n",
       " 120539,\n",
       " 72998,\n",
       " 150407,\n",
       " 120029,\n",
       " 37619,\n",
       " 86583,\n",
       " 139981,\n",
       " 109914,\n",
       " 144623,\n",
       " 88246,\n",
       " 19030,\n",
       " 158889,\n",
       " 31450,\n",
       " 125710,\n",
       " 4248,\n",
       " 153680,\n",
       " 138289,\n",
       " 70578,\n",
       " 48595,\n",
       " 95096,\n",
       " 85923,\n",
       " 32427,\n",
       " 7497,\n",
       " 104689,\n",
       " 11699,\n",
       " 94486,\n",
       " 149837,\n",
       " 152437,\n",
       " 64077,\n",
       " 140262,\n",
       " 69921,\n",
       " 78755,\n",
       " 86349,\n",
       " 69198,\n",
       " 11782,\n",
       " 50017,\n",
       " 53841,\n",
       " 54451,\n",
       " 155537,\n",
       " 40955,\n",
       " 9501,\n",
       " 23296,\n",
       " 39861,\n",
       " 137238,\n",
       " 51078,\n",
       " 30978,\n",
       " 59977,\n",
       " 58743,\n",
       " 141392,\n",
       " 155212,\n",
       " 132432,\n",
       " 98297,\n",
       " 89907,\n",
       " 49126,\n",
       " 32111,\n",
       " 19324,\n",
       " 45202,\n",
       " 135226,\n",
       " 35066,\n",
       " 92005,\n",
       " 92106,\n",
       " 21404,\n",
       " 145154,\n",
       " 11446,\n",
       " 29598,\n",
       " 2574,\n",
       " 26051,\n",
       " 142869,\n",
       " 95240,\n",
       " 29094,\n",
       " 14506,\n",
       " 107015,\n",
       " 39545,\n",
       " 19661,\n",
       " 119355,\n",
       " 59334,\n",
       " 111286,\n",
       " 99523,\n",
       " 110412,\n",
       " 139575,\n",
       " 62169,\n",
       " 132548,\n",
       " 7885,\n",
       " 23032,\n",
       " 62929,\n",
       " 94907,\n",
       " 69021,\n",
       " 66908,\n",
       " 142948,\n",
       " 73580,\n",
       " 18594,\n",
       " 136823,\n",
       " 126769,\n",
       " 90986,\n",
       " 118870,\n",
       " 155362,\n",
       " 46040,\n",
       " 97904,\n",
       " 156027,\n",
       " 119703,\n",
       " 97503,\n",
       " 33058,\n",
       " 117155,\n",
       " 156546,\n",
       " 13960,\n",
       " 18037,\n",
       " 81420,\n",
       " 37071,\n",
       " 124448,\n",
       " 17739,\n",
       " 25836,\n",
       " 152592,\n",
       " 73499,\n",
       " 159742,\n",
       " 82152,\n",
       " 51751,\n",
       " 32471,\n",
       " 34967,\n",
       " 37907,\n",
       " 80883,\n",
       " 37282,\n",
       " 47729,\n",
       " 1775,\n",
       " 57310,\n",
       " 20537,\n",
       " 19466,\n",
       " 49916,\n",
       " 42729,\n",
       " 58714,\n",
       " 23508,\n",
       " 94601,\n",
       " 145535,\n",
       " 67892,\n",
       " 24913,\n",
       " 27502,\n",
       " 32584,\n",
       " 53109,\n",
       " 95955,\n",
       " 94640,\n",
       " 87857,\n",
       " 19193,\n",
       " 157813,\n",
       " 63952,\n",
       " 11242,\n",
       " 52092,\n",
       " 88683,\n",
       " 71421,\n",
       " 96717,\n",
       " 158627,\n",
       " 8644,\n",
       " 66807,\n",
       " 72709,\n",
       " 59163,\n",
       " 44604,\n",
       " 26595,\n",
       " 135400,\n",
       " 55181,\n",
       " 6436,\n",
       " 96560,\n",
       " 16759,\n",
       " 116495,\n",
       " 142442,\n",
       " 80705,\n",
       " 134640,\n",
       " 25694,\n",
       " 48916,\n",
       " 59939,\n",
       " 98700,\n",
       " 116248,\n",
       " 117099,\n",
       " 44796,\n",
       " 136467,\n",
       " 45910,\n",
       " 129023,\n",
       " 146667,\n",
       " 24575,\n",
       " 49569,\n",
       " 113552,\n",
       " 112386,\n",
       " 107787,\n",
       " 979,\n",
       " 89949,\n",
       " 133582,\n",
       " 54956,\n",
       " 56710,\n",
       " 114529,\n",
       " 124407,\n",
       " 134221,\n",
       " 104726,\n",
       " 147137,\n",
       " 85030,\n",
       " 159335,\n",
       " 105504,\n",
       " 55729,\n",
       " 6063,\n",
       " 142045,\n",
       " 41604,\n",
       " 51461,\n",
       " 41637,\n",
       " 9962,\n",
       " 141942,\n",
       " 96357,\n",
       " 58038,\n",
       " 88893,\n",
       " 116067,\n",
       " 56529,\n",
       " 74803,\n",
       " 26064,\n",
       " 108140,\n",
       " 137297,\n",
       " 138793,\n",
       " 112621,\n",
       " 97002,\n",
       " 58984,\n",
       " 115730,\n",
       " 43292,\n",
       " 8421,\n",
       " 127719,\n",
       " 141933,\n",
       " 148754,\n",
       " 68876,\n",
       " 57677,\n",
       " 98802,\n",
       " 141254,\n",
       " 25784,\n",
       " 61309,\n",
       " 137787,\n",
       " 54704,\n",
       " 78530,\n",
       " 103439,\n",
       " 96884,\n",
       " 158336,\n",
       " 48134,\n",
       " 145928,\n",
       " 45418,\n",
       " 135737,\n",
       " 53616,\n",
       " 72067,\n",
       " 63663,\n",
       " 5221,\n",
       " 4111,\n",
       " 152935,\n",
       " 25282,\n",
       " 148359,\n",
       " 83563,\n",
       " 132599,\n",
       " 43005,\n",
       " 26522,\n",
       " 116568,\n",
       " 93118,\n",
       " 116775,\n",
       " 92084,\n",
       " 9611,\n",
       " 52156,\n",
       " 118646,\n",
       " 151641,\n",
       " 79445,\n",
       " 155218,\n",
       " 154817,\n",
       " 159016,\n",
       " 42248,\n",
       " 104864,\n",
       " 46814,\n",
       " 70036,\n",
       " 140034,\n",
       " 141415,\n",
       " 41666,\n",
       " 15988,\n",
       " 103485,\n",
       " 110295,\n",
       " 116994,\n",
       " 3633,\n",
       " 69330,\n",
       " 136052,\n",
       " 100628,\n",
       " 12435,\n",
       " 25955,\n",
       " 132860,\n",
       " 150876,\n",
       " 123928,\n",
       " 98170,\n",
       " 143302,\n",
       " 136174,\n",
       " 150101,\n",
       " 27628,\n",
       " 44817,\n",
       " 22629,\n",
       " 58985,\n",
       " 65080,\n",
       " 22137,\n",
       " 68800,\n",
       " 112993,\n",
       " 78747,\n",
       " 156519,\n",
       " 68669,\n",
       " 9248,\n",
       " 135716,\n",
       " 27330,\n",
       " 70146,\n",
       " 147662,\n",
       " 22775,\n",
       " 110154,\n",
       " 35474,\n",
       " 15678,\n",
       " 20668,\n",
       " 67531,\n",
       " 19676,\n",
       " 16360,\n",
       " 72013,\n",
       " 16456,\n",
       " 73708,\n",
       " 158125,\n",
       " 73307,\n",
       " 124866,\n",
       " 14279,\n",
       " 135734,\n",
       " 104862,\n",
       " 131414,\n",
       " 54938,\n",
       " 136365,\n",
       " 60060,\n",
       " 14338,\n",
       " 59786,\n",
       " 76620,\n",
       " 70166,\n",
       " 129340,\n",
       " 147895,\n",
       " 22469,\n",
       " 107684,\n",
       " 77806,\n",
       " 127077,\n",
       " 16583,\n",
       " 128049,\n",
       " 132908,\n",
       " 99693,\n",
       " 92643,\n",
       " 64314,\n",
       " 28022,\n",
       " 119089,\n",
       " 11012,\n",
       " 38684,\n",
       " 95833,\n",
       " 90749,\n",
       " 71139,\n",
       " 45965,\n",
       " 132292,\n",
       " 55299,\n",
       " 8899,\n",
       " 51598,\n",
       " 152318,\n",
       " 100116,\n",
       " 157973,\n",
       " 76565,\n",
       " 74300,\n",
       " 32636,\n",
       " 34495,\n",
       " 53173,\n",
       " 32681,\n",
       " 128659,\n",
       " 91986,\n",
       " 46681,\n",
       " 17586,\n",
       " 112670,\n",
       " 146827,\n",
       " 121144,\n",
       " 26262,\n",
       " 45151,\n",
       " 68478,\n",
       " 28221,\n",
       " 12938,\n",
       " 90679,\n",
       " 119594,\n",
       " 47856,\n",
       " 34195,\n",
       " 81263,\n",
       " 122789,\n",
       " 1813,\n",
       " 141165,\n",
       " 80838,\n",
       " 148286,\n",
       " 73379,\n",
       " 119779,\n",
       " 18465,\n",
       " 19302,\n",
       " 41519,\n",
       " 158453,\n",
       " 69315,\n",
       " 37875,\n",
       " 102179,\n",
       " 12299,\n",
       " 16878,\n",
       " 158090,\n",
       " 139766,\n",
       " 62170,\n",
       " 112977,\n",
       " 139158,\n",
       " 115901,\n",
       " 158672,\n",
       " 119090,\n",
       " 83390,\n",
       " 75710,\n",
       " 86183,\n",
       " 11294,\n",
       " 100100,\n",
       " 5545,\n",
       " 64872,\n",
       " 87232,\n",
       " 130114,\n",
       " 116773,\n",
       " 41411,\n",
       " 133234,\n",
       " 126707,\n",
       " 78952,\n",
       " 67857,\n",
       " 83144,\n",
       " 122420,\n",
       " 114780,\n",
       " 141851,\n",
       " 58728,\n",
       " 120447,\n",
       " 146235,\n",
       " 97949,\n",
       " 45743,\n",
       " 113028,\n",
       " 78205,\n",
       " 67397,\n",
       " 102495,\n",
       " 66916,\n",
       " 51103,\n",
       " 42195,\n",
       " 3474,\n",
       " 29520,\n",
       " 11356,\n",
       " 28178,\n",
       " 27688,\n",
       " 22316,\n",
       " 57488,\n",
       " 91651,\n",
       " 74744,\n",
       " 4889,\n",
       " 38613,\n",
       " 52796,\n",
       " 156797,\n",
       " 46060,\n",
       " 127119,\n",
       " 108511,\n",
       " 109245,\n",
       " 104103,\n",
       " 74332,\n",
       " 119202,\n",
       " 35786,\n",
       " 4012,\n",
       " 129201,\n",
       " 12235,\n",
       " 106086,\n",
       " 78043,\n",
       " 140614,\n",
       " 7144,\n",
       " 69450,\n",
       " 40577,\n",
       " 115039,\n",
       " 150158,\n",
       " 134465,\n",
       " 124479,\n",
       " 63504,\n",
       " 154899,\n",
       " 64633,\n",
       " 118871,\n",
       " 135855,\n",
       " 7058,\n",
       " 152071,\n",
       " 112281,\n",
       " 157623,\n",
       " 152532,\n",
       " 64060,\n",
       " 76878,\n",
       " 144757,\n",
       " 128628,\n",
       " 113192,\n",
       " 100111,\n",
       " 68419,\n",
       " 71966,\n",
       " 119687,\n",
       " 84766,\n",
       " 27856,\n",
       " 112135,\n",
       " 52506,\n",
       " 41258,\n",
       " 103911,\n",
       " 78972,\n",
       " 140999,\n",
       " 25426,\n",
       " 137775,\n",
       " 101501,\n",
       " 108091,\n",
       " 50984,\n",
       " 146292,\n",
       " 26145,\n",
       " 10217,\n",
       " 112940,\n",
       " 28778,\n",
       " 61896,\n",
       " 4865,\n",
       " 95664,\n",
       " 147905,\n",
       " 26860,\n",
       " 50567,\n",
       " 140709,\n",
       " 1274,\n",
       " 90398,\n",
       " 100051,\n",
       " 21859,\n",
       " 5415,\n",
       " 35714,\n",
       " 7313,\n",
       " 6273,\n",
       " 141123,\n",
       " 70123,\n",
       " 136133,\n",
       " 58752,\n",
       " 130019,\n",
       " 42483,\n",
       " 7148,\n",
       " 2410,\n",
       " 81418,\n",
       " 34477,\n",
       " 19987,\n",
       " 6088,\n",
       " 151514,\n",
       " 58794,\n",
       " 142149,\n",
       " 93782,\n",
       " 139880,\n",
       " 87990,\n",
       " 17127,\n",
       " 73807,\n",
       " 134379,\n",
       " 143722,\n",
       " 88208,\n",
       " 5144,\n",
       " 45199,\n",
       " 84370,\n",
       " 18875,\n",
       " 144136,\n",
       " 142421,\n",
       " 13938,\n",
       " 19757,\n",
       " 2717,\n",
       " 144134,\n",
       " 155840,\n",
       " 115367,\n",
       " 58482,\n",
       " 68332,\n",
       " 27838,\n",
       " 114881,\n",
       " 138810,\n",
       " 91758,\n",
       " 92126,\n",
       " 109512,\n",
       " 31293,\n",
       " 140178,\n",
       " 108161,\n",
       " 157874,\n",
       " 104764,\n",
       " 36594,\n",
       " 111871,\n",
       " 95374,\n",
       " 111678,\n",
       " 123276,\n",
       " 151888,\n",
       " 120999,\n",
       " 5963,\n",
       " 146520,\n",
       " 23856,\n",
       " 66546,\n",
       " 106318,\n",
       " 157156,\n",
       " 86219,\n",
       " 74162,\n",
       " 53121,\n",
       " 67475,\n",
       " 81458,\n",
       " 132544,\n",
       " 132609,\n",
       " 77729,\n",
       " 9420,\n",
       " 559,\n",
       " 83119,\n",
       " 90163,\n",
       " 114631,\n",
       " 27105,\n",
       " 15941,\n",
       " 93162,\n",
       " 40603,\n",
       " 109643,\n",
       " 113500,\n",
       " 23343,\n",
       " 133113,\n",
       " 321,\n",
       " 89539,\n",
       " 122289,\n",
       " 49867,\n",
       " 155339,\n",
       " 23554,\n",
       " 83961,\n",
       " 80701,\n",
       " 21463,\n",
       " 131,\n",
       " 78669,\n",
       " 145317,\n",
       " 34821,\n",
       " 113623,\n",
       " 46081,\n",
       " 15177,\n",
       " 117591,\n",
       " 159064,\n",
       " 10197,\n",
       " 105724,\n",
       " 35736,\n",
       " 102190,\n",
       " 7845,\n",
       " 85249,\n",
       " 100491,\n",
       " 196,\n",
       " 151800,\n",
       " 40264,\n",
       " 96398,\n",
       " 92422,\n",
       " 76471,\n",
       " 23279,\n",
       " 141511,\n",
       " 104879,\n",
       " 92982,\n",
       " 51319,\n",
       " 88524,\n",
       " 144210,\n",
       " 1221,\n",
       " 51600,\n",
       " 67001,\n",
       " 95258,\n",
       " 23262,\n",
       " 13484,\n",
       " 106233,\n",
       " 155303,\n",
       " 44668,\n",
       " 64918,\n",
       " 97779,\n",
       " 152430,\n",
       " 84588,\n",
       " 156544,\n",
       " 11106,\n",
       " 155461,\n",
       " 55156,\n",
       " 25326,\n",
       " 35453,\n",
       " 32321,\n",
       " 103638,\n",
       " 116654,\n",
       " 127242,\n",
       " 155814,\n",
       " 34451,\n",
       " 72947,\n",
       " 115596,\n",
       " 49032,\n",
       " 72029,\n",
       " 95597,\n",
       " 147334,\n",
       " 91107,\n",
       " 46678,\n",
       " 60256,\n",
       " 57140,\n",
       " 45391,\n",
       " 88077,\n",
       " 56027,\n",
       " 57972,\n",
       " 51559,\n",
       " 153950,\n",
       " 10437,\n",
       " 1304,\n",
       " 75507,\n",
       " 123287,\n",
       " 89262,\n",
       " 34484,\n",
       " 24399,\n",
       " 27106,\n",
       " 22884,\n",
       " 34390,\n",
       " 22453,\n",
       " 123116,\n",
       " 12838,\n",
       " 159589,\n",
       " 11279,\n",
       " 157533,\n",
       " 1632,\n",
       " 79541,\n",
       " 56978,\n",
       " 39243,\n",
       " 111717,\n",
       " 71406,\n",
       " 118235,\n",
       " 117272,\n",
       " 150689,\n",
       " 128397,\n",
       " 131774,\n",
       " 42764,\n",
       " 68207,\n",
       " 139146,\n",
       " 14159,\n",
       " 68477,\n",
       " 10518,\n",
       " 46396,\n",
       " 78971,\n",
       " 75638,\n",
       " 62320,\n",
       " 95190,\n",
       " 20889,\n",
       " 114680,\n",
       " 40060,\n",
       " 79140,\n",
       " 49033,\n",
       " 96209,\n",
       " 42741,\n",
       " 54114,\n",
       " 23290,\n",
       " 153764,\n",
       " 59213,\n",
       " 93250,\n",
       " 115116,\n",
       " 43075,\n",
       " 37922,\n",
       " 24273,\n",
       " 144323,\n",
       " 10637,\n",
       " 45617,\n",
       " 66767,\n",
       " 35896,\n",
       " 95130,\n",
       " 101453,\n",
       " 140589,\n",
       " 9825,\n",
       " 91117,\n",
       " 105756,\n",
       " 66116,\n",
       " 65973,\n",
       " 34781,\n",
       " 156767,\n",
       " 138702,\n",
       " 88334,\n",
       " 1075,\n",
       " 23060,\n",
       " 12807,\n",
       " 21765,\n",
       " 24956,\n",
       " 79432,\n",
       " 26871,\n",
       " 11436,\n",
       " 97849,\n",
       " 32099,\n",
       " 71971,\n",
       " 48795,\n",
       " 153005,\n",
       " 4879,\n",
       " 108407,\n",
       " 100457,\n",
       " 75244,\n",
       " 120449,\n",
       " 68276,\n",
       " 9371,\n",
       " 12348,\n",
       " 119864,\n",
       " 12423,\n",
       " 83146,\n",
       " 113755,\n",
       " 133621,\n",
       " 113600,\n",
       " 76375,\n",
       " 114860,\n",
       " 81876,\n",
       " 92793,\n",
       " 7141,\n",
       " 94186,\n",
       " 109387,\n",
       " 113412,\n",
       " 127780,\n",
       " 48912,\n",
       " 58154,\n",
       " 130022,\n",
       " 91773,\n",
       " 114071,\n",
       " 154495,\n",
       " 141175,\n",
       " 151542,\n",
       " 54409,\n",
       " 32041,\n",
       " 72482,\n",
       " 137823,\n",
       " 106910,\n",
       " 118227,\n",
       " 36883,\n",
       " 16755,\n",
       " 9651,\n",
       " 68994,\n",
       " 1739,\n",
       " 62794,\n",
       " 38681,\n",
       " 71986,\n",
       " 87398,\n",
       " 89372,\n",
       " 12606,\n",
       " 90684,\n",
       " 28567,\n",
       " 126272,\n",
       " 32866,\n",
       " 43505,\n",
       " 19877,\n",
       " 39055,\n",
       " 142084,\n",
       " 157706,\n",
       " 96653,\n",
       " 103284,\n",
       " 152391,\n",
       " 37713,\n",
       " 149117,\n",
       " 16484,\n",
       " 108856,\n",
       " 133167,\n",
       " 114444,\n",
       " 126719,\n",
       " 62365,\n",
       " 75460,\n",
       " 85990,\n",
       " 59143,\n",
       " 92281,\n",
       " 104456,\n",
       " 48973,\n",
       " 70515,\n",
       " 130692,\n",
       " 84947,\n",
       " 94698,\n",
       " 62315,\n",
       " 40309,\n",
       " 66352,\n",
       " 144198,\n",
       " 116406,\n",
       " 42987,\n",
       " 66902,\n",
       " 92892,\n",
       " 26999,\n",
       " 77468,\n",
       " 103972,\n",
       " 143952,\n",
       " 76104,\n",
       " 69951,\n",
       " 159088,\n",
       " 49638,\n",
       " 151319,\n",
       " 65351,\n",
       " 61185,\n",
       " 32673,\n",
       " 124786,\n",
       " 3542,\n",
       " 107318,\n",
       " 81581,\n",
       " 140807,\n",
       " 29694,\n",
       " 59187,\n",
       " 17428,\n",
       " 57218,\n",
       " 114459,\n",
       " 50365,\n",
       " 121323,\n",
       " 38351,\n",
       " 26144,\n",
       " 107205,\n",
       " 10963,\n",
       " 140753,\n",
       " 50028,\n",
       " 94548,\n",
       " 61528,\n",
       " 75603,\n",
       " 11697,\n",
       " 108989,\n",
       " 103582,\n",
       " 43915,\n",
       " 77643,\n",
       " 138029,\n",
       " 70173,\n",
       " 16275,\n",
       " 149062,\n",
       " 82804,\n",
       " 55929,\n",
       " 48010,\n",
       " 51757,\n",
       " 32867,\n",
       " 134485,\n",
       " 137759,\n",
       " 80090,\n",
       " 84720,\n",
       " 82647,\n",
       " 83309,\n",
       " 131557,\n",
       " 79552,\n",
       " 36748,\n",
       " 106096,\n",
       " 76661,\n",
       " 121235,\n",
       " 84990,\n",
       " 140265,\n",
       " 84310,\n",
       " 60589,\n",
       " 10945,\n",
       " 14170,\n",
       " 135873,\n",
       " 103415,\n",
       " 133419,\n",
       " 115254,\n",
       " 66601,\n",
       " 94282,\n",
       " 53572,\n",
       " 80442,\n",
       " 49458,\n",
       " 92663,\n",
       " 18806,\n",
       " 26396,\n",
       " 141929,\n",
       " 83237,\n",
       " 148100,\n",
       " 32937,\n",
       " 52826,\n",
       " 87567,\n",
       " 124156,\n",
       " 35507,\n",
       " 140878,\n",
       " 79866,\n",
       " 138551,\n",
       " 48013,\n",
       " 13978,\n",
       " 45907,\n",
       " 127018,\n",
       " 2658,\n",
       " 33311,\n",
       " 6836,\n",
       " 121290,\n",
       " 158694,\n",
       " 69873,\n",
       " 85476,\n",
       " 112626,\n",
       " 154359,\n",
       " 78810,\n",
       " 122820,\n",
       " 9079,\n",
       " 74089,\n",
       " 23970,\n",
       " 58600,\n",
       " 89695,\n",
       " 149272,\n",
       " 66860,\n",
       " 14109,\n",
       " 51989,\n",
       " 86734,\n",
       " 80965,\n",
       " 15287,\n",
       " 5435,\n",
       " 106083,\n",
       " 76572,\n",
       " 152072,\n",
       " 56752,\n",
       " 31238,\n",
       " 93217,\n",
       " 54323,\n",
       " 156813,\n",
       " 116069,\n",
       " 139792,\n",
       " 9521,\n",
       " 120034,\n",
       " 150670,\n",
       " 53552,\n",
       " 154930,\n",
       " 85036,\n",
       " 108592,\n",
       " 63276,\n",
       " 67289,\n",
       " 146500,\n",
       " 24813,\n",
       " 36649,\n",
       " 130786,\n",
       " 118833,\n",
       " 154585,\n",
       " 47973,\n",
       " 141525,\n",
       " 86007,\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ga_features = feature_selection.get_GA_features(X=X, Y=Y)\n",
    "ga_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gera as combinações de parâmetros para rodar a rede neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_classifier = MLPClassifier(solver=\"sgd\", learning_rate=\"constant\")\n",
    "\n",
    "parameters = [{\"hidden_layer_sizes\":[(10,), (25,), #variando o tamanho de uma única camada escondida\n",
    "                                     (10, 15), # duas camadas ocultas com 10 e 15 neuronios cada\n",
    "                                     (10, 10, 10)], # três camadas ocultas com 10 neuronios cada\n",
    "               \"learning_rate_init\":[0.001, 0.01, 0.1, 0.5, 0.9],\n",
    "               \"activation\":['logistic', 'relu']}]\n",
    "scoring = [\"accuracy\", \"precision\", \"recall\", \"roc_auc\"]\n",
    "\n",
    "result_column_list= ['param_activation',\n",
    "                     'param_hidden_layer_sizes',\n",
    "                     'param_learning_rate_init',\n",
    "                     'mean_test_accuracy',\n",
    "                     'mean_test_precision',\n",
    "                     'mean_test_recall',\n",
    "                     'mean_test_roc_auc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Roda todas as combinações da rede neural para todas as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_full = GridSearchCV(mlp_classifier, parameters, verbose=5, n_jobs=-1, cv=3, scoring=scoring, refit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 40 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed: 21.4min\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed: 34.6min finished\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='sgd', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid=[{'hidden_layer_sizes': [(10,), (25,), (10, 15), (10, 10, 10)], 'learning_rate_init': [0.001, 0.01, 0.1, 0.5, 0.9], 'activation': ['logistic', 'relu']}],\n",
       "       pre_dispatch='2*n_jobs', refit=False, return_train_score='warn',\n",
       "       scoring=['accuracy', 'precision', 'recall', 'roc_auc'], verbose=5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_full.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_accuracy'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_accuracy'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_accuracy'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_accuracy'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_accuracy'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_precision'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_precision'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_precision'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_precision'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_precision'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_recall'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_recall'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_recall'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_recall'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_recall'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_roc_auc'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_roc_auc'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_roc_auc'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_roc_auc'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_roc_auc'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "full_result = pd.DataFrame(grid_search_full.cv_results_)[result_column_list]\n",
    "full_result.to_csv(\"Sample_400_Full.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Roda todas as combinações da rede neural para as features extraídas pelo PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_pca = GridSearchCV(mlp_classifier, parameters, verbose=5, n_jobs=-1, cv=3, scoring=scoring, refit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 40 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:    2.6s finished\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='sgd', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid=[{'hidden_layer_sizes': [(10,), (25,), (10, 15), (10, 10, 10)], 'learning_rate_init': [0.001, 0.01, 0.1, 0.5, 0.9], 'activation': ['logistic', 'relu']}],\n",
       "       pre_dispatch='2*n_jobs', refit=False, return_train_score='warn',\n",
       "       scoring=['accuracy', 'precision', 'recall', 'roc_auc'], verbose=5)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_pca.fit(pca_components, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_accuracy'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_accuracy'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_accuracy'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_accuracy'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_accuracy'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_precision'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_precision'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_precision'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_precision'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_precision'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_recall'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_recall'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_recall'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_recall'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_recall'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_roc_auc'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_roc_auc'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_roc_auc'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_roc_auc'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_roc_auc'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "pca_result = pd.DataFrame(grid_search_pca.cv_results_)[result_column_list]\n",
    "pca_result.to_csv(\"Sample_400_PCA2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Roda todas as combinações da rede neural para as features selecionadas pela Informação Mútua (MIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_mic = GridSearchCV(mlp_classifier, parameters, verbose=5, n_jobs=-1, cv=3, scoring=scoring, refit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 40 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:   10.3s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:   50.0s\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:  1.4min finished\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='sgd', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid=[{'hidden_layer_sizes': [(10,), (25,), (10, 15), (10, 10, 10)], 'learning_rate_init': [0.001, 0.01, 0.1, 0.5, 0.9], 'activation': ['logistic', 'relu']}],\n",
       "       pre_dispatch='2*n_jobs', refit=False, return_train_score='warn',\n",
       "       scoring=['accuracy', 'precision', 'recall', 'roc_auc'], verbose=5)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_mic.fit(X[mic_features], Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_accuracy'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_accuracy'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_accuracy'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_accuracy'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_accuracy'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_precision'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_precision'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_precision'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_precision'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_precision'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_recall'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_recall'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_recall'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_recall'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_recall'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_roc_auc'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_roc_auc'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_roc_auc'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_roc_auc'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_roc_auc'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "mic_result = pd.DataFrame(grid_search_mic.cv_results_)[result_column_list]\n",
    "mic_result.to_csv(\"Sample_400_MIC.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Roda todas as combinações da rede neural para as features selecionadas pelo Algoritmo Genético (GA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_ga = GridSearchCV(mlp_classifier, parameters, verbose=5, n_jobs=-1, cv=3, scoring=scoring, refit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 40 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:    2.4s finished\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='sgd', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid=[{'hidden_layer_sizes': [(10,), (25,), (10, 15), (10, 10, 10)], 'learning_rate_init': [0.001, 0.01, 0.1, 0.5, 0.9], 'activation': ['logistic', 'relu']}],\n",
       "       pre_dispatch='2*n_jobs', refit=False, return_train_score='warn',\n",
       "       scoring=['accuracy', 'precision', 'recall', 'roc_auc'], verbose=5)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_ga.fit(pca_components, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_accuracy'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_accuracy'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_accuracy'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_accuracy'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_accuracy'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_precision'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_precision'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_precision'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_precision'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_precision'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_recall'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_recall'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_recall'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_recall'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_recall'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_roc_auc'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_roc_auc'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_roc_auc'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_roc_auc'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\theus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_roc_auc'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "ga_result = pd.DataFrame(grid_search_ga.cv_results_)[result_column_list]\n",
    "ga_result.to_csv(\"Sample_400_GA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
